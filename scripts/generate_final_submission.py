#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± - 8-bit ì–‘ìí™” + ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ + í•˜ì´ë¸Œë¦¬ë“œ RAG
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import logging
import time
from datetime import datetime
from scripts.simple_improved_prompt import SimpleImprovedPrompt
from scripts.improved_answer_extraction import ImprovedAnswerExtractor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from packages.rag.rag_pipeline import RAGPipeline
from packages.rag.embeddings.kure_embedder import KUREEmbedder
from packages.llm.qwen_quantized import QuantizedQwenLLM

def validate_answer_length(answer: str, question_id: str, is_mc: bool) -> str:
    """
    ë‹µë³€ ê¸¸ì´ ê²€ì¦ ë° ìˆ˜ì •
    
    Args:
        answer: ì›ë³¸ ë‹µë³€
        question_id: ì§ˆë¬¸ ID
        is_mc: ê°ê´€ì‹ ì—¬ë¶€
        
    Returns:
        ê²€ì¦ëœ ë‹µë³€
    """
    if is_mc:
        # ê°ê´€ì‹: 1-2ìë¦¬ ìˆ«ìì—¬ì•¼ í•¨
        if len(answer) > 2:
            logger.warning(f"{question_id}: ê°ê´€ì‹ ë‹µë³€ì´ ë„ˆë¬´ ê¹€ ({len(answer)}ì) -> '1'ë¡œ ìˆ˜ì •")
            return "1"
        return answer
    else:
        # ì£¼ê´€ì‹: ìµœëŒ€ 500ìë¡œ ì œí•œ
        if len(answer) > 500:
            logger.warning(f"{question_id}: ì£¼ê´€ì‹ ë‹µë³€ì´ ë„ˆë¬´ ê¹€ ({len(answer)}ì) -> 500ìë¡œ ìë¦„")
            return answer[:500]
        # ë„ˆë¬´ ì§§ì€ ë‹µë³€ë„ í™•ì¸
        if len(answer) < 10:
            logger.warning(f"{question_id}: ì£¼ê´€ì‹ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ ({len(answer)}ì)")
        return answer

def generate_final_submission():
    """ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„±"""
    
    print("="*80)
    print("ğŸš€ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì‹œì‘")
    print("="*80)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ì„¤ì •: 8-bit ì–‘ìí™” + í•˜ì´ë¸Œë¦¬ë“œ RAG + ê°œì„ ëœ í”„ë¡¬í”„íŠ¸")
    
    start_time = time.time()
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test_df = pd.read_csv("data/competition/test.csv")
    total_questions = len(test_df)
    print(f"   ì´ {total_questions}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
    
    # 2. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    print("\nğŸ”§ ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”...")
    
    # Embedder
    print("   1. KURE Embedder ì´ˆê¸°í™”...")
    embedder = KUREEmbedder(
        model_name="nlpai-lab/KURE-v1",
        batch_size=32,
        show_progress=False
    )
    print("      âœ… Embedder ë¡œë“œ ì™„ë£Œ")
    
    # RAG Pipeline
    print("   2. í•˜ì´ë¸Œë¦¬ë“œ RAG Pipeline ì´ˆê¸°í™”...")
    rag_pipeline = RAGPipeline(
        embedder=embedder,
        retriever_type="hybrid",
        knowledge_base_path="data/rag/knowledge_base_fixed",
        enable_reranking=False,
        initial_retrieve_k=10,
        final_k=5
    )
    print("      âœ… í•˜ì´ë¸Œë¦¬ë“œ RAG Pipeline ì´ˆê¸°í™” ì™„ë£Œ")
    
    # LLM (8-bit)
    print("   3. Qwen2.5-7B-Instruct (8-bit) ì´ˆê¸°í™”...")
    llm_start = time.time()
    llm = QuantizedQwenLLM(
        model_id="Qwen/Qwen2.5-7B-Instruct",
        quantization_type="8bit",
        cache_dir="./models"
    )
    llm_init_time = time.time() - llm_start
    print(f"      âœ… 8-bit LLM ë¡œë“œ ì™„ë£Œ ({llm_init_time:.1f}ì´ˆ)")
    
    memory_info = llm.get_memory_footprint()
    print(f"      GPU ë©”ëª¨ë¦¬: {memory_info}")
    
    # í”„ë¡¬í”„íŠ¸ ë° ì¶”ì¶œê¸°
    prompt_generator = SimpleImprovedPrompt()
    extractor = ImprovedAnswerExtractor()
    
    # 3. ì „ì²´ ì§ˆë¬¸ ì²˜ë¦¬
    print(f"\nğŸ¤– ì „ì²´ {total_questions}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘...")
    print("="*80)
    
    results = []
    failed_count = 0
    mc_count = 0
    oe_count = 0
    total_inference_time = 0
    
    # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸
    checkpoints = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 515]
    
    for idx, row in test_df.iterrows():
        question_id = row['ID']
        question = row['Question']
        
        # ì§„í–‰ë¥  í‘œì‹œ
        current_num = idx + 1
        if current_num in checkpoints or current_num % 25 == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / current_num if current_num > 0 else 0
            remaining = (total_questions - current_num) * avg_time
            print(f"\nğŸ“Š ì§„í–‰ë¥ : {current_num}/{total_questions} ({current_num/total_questions*100:.1f}%)")
            print(f"   ê²½ê³¼ ì‹œê°„: {elapsed/60:.1f}ë¶„, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„")
            print(f"   ì‹¤íŒ¨: {failed_count}ê°œ, ê°ê´€ì‹: {mc_count}ê°œ, ì£¼ê´€ì‹: {oe_count}ê°œ")
        
        try:
            # RAG ê²€ìƒ‰
            contexts_list = rag_pipeline.retrieve(
                query=question,
                top_k=5,
                use_reranking=False
            )
            
            if contexts_list:
                contexts = [doc.get('content', '') for doc in contexts_list]
            else:
                contexts = []
            
            # ê°ê´€ì‹ ì—¬ë¶€ íŒë‹¨
            is_mc = any(f"{i}." in question or f"{i} " in question for i in range(1, 10))
            if is_mc:
                mc_count += 1
            else:
                oe_count += 1
            
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompt_generator.create_improved_prompt(
                question=question,
                contexts=contexts,
                is_mc=is_mc
            )
            
            # LLM ìƒì„±
            inference_start = time.time()
            response = llm.generate_optimized(
                prompt=prompt,
                max_new_tokens=128 if is_mc else 256,
                temperature=0.1,
                use_cache=False
            )
            inference_time = time.time() - inference_start
            total_inference_time += inference_time
            
            # ê°œì„ ëœ ë‹µë³€ ì¶”ì¶œ
            if is_mc:
                max_choice = extractor.extract_choice_range(question)
                answer = extractor.extract_answer_from_response(
                    response=response,
                    max_choice=max_choice,
                    question=question
                )
            else:
                answer = extractor.extract_open_ended_answer(response)
            
            # ë‹µë³€ ê¸¸ì´ ê²€ì¦
            answer = validate_answer_length(answer, question_id, is_mc)
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ {question_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            answer = "1" if is_mc else "ì²˜ë¦¬ ì‹¤íŒ¨"
            failed_count += 1
        
        results.append({
            "ID": question_id,
            "Answer": answer
        })
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 100ê°œë§ˆë‹¤)
        if current_num % 100 == 0:
            import torch
            torch.cuda.empty_cache()
    
    total_time = time.time() - start_time
    avg_inference_time = total_inference_time / total_questions
    
    # 4. ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # submission.csv ìƒì„±
    submission_df = pd.DataFrame(results)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    submission_path = f"submission_8bit_{timestamp}.csv"
    
    submission_df.to_csv(submission_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… ì œì¶œíŒŒì¼ ì €ì¥: {submission_path}")
    
    # ë°±ì—… ì €ì¥
    backup_path = f"test_results/submission_8bit_backup_{timestamp}.csv"
    submission_df.to_csv(backup_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… ë°±ì—…íŒŒì¼ ì €ì¥: {backup_path}")
    
    # 5. ìµœì¢… í†µê³„
    print(f"\n" + "="*80)
    print("ğŸ‰ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì™„ë£Œ!")
    print("="*80)
    
    print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    print(f"   ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}ê°œ")
    print(f"   ê°ê´€ì‹: {mc_count}ê°œ ({mc_count/total_questions*100:.1f}%)")
    print(f"   ì£¼ê´€ì‹: {oe_count}ê°œ ({oe_count/total_questions*100:.1f}%)")
    print(f"   ì²˜ë¦¬ ì‹¤íŒ¨: {failed_count}ê°œ ({failed_count/total_questions*100:.1f}%)")
    
    print(f"\nâ±ï¸ ì‹œê°„ í†µê³„:")
    print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/60:.1f}ë¶„")
    print(f"   LLM ì´ˆê¸°í™”: {llm_init_time:.1f}ì´ˆ")
    print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.2f}ì´ˆ/ì§ˆë¬¸")
    print(f"   ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time/60:.1f}ë¶„")
    
    print(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:")
    final_memory = llm.get_memory_footprint()
    print(f"   ìµœì¢… GPU ë©”ëª¨ë¦¬: {final_memory}")
    print(f"   ì–‘ìí™” ë°©ì‹: 8-bit")
    print(f"   RAG ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (BM25 + Vector)")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"   ë©”ì¸ ì œì¶œíŒŒì¼: {submission_path}")
    print(f"   ë°±ì—… íŒŒì¼: {backup_path}")
    
    # 6. íŒŒì¼ ê²€ì¦
    print(f"\nğŸ” ì œì¶œíŒŒì¼ ê²€ì¦...")
    try:
        # íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸
        verify_df = pd.read_csv(submission_path)
        
        # ê¸°ë³¸ ê²€ì¦
        assert len(verify_df) == total_questions, f"í–‰ ìˆ˜ ë¶ˆì¼ì¹˜: {len(verify_df)} != {total_questions}"
        assert list(verify_df.columns) == ['ID', 'Answer'], f"ì»¬ëŸ¼ëª… ì˜¤ë¥˜: {list(verify_df.columns)}"
        assert verify_df['ID'].isnull().sum() == 0, "IDì— null ê°’ ì¡´ì¬"
        assert verify_df['Answer'].isnull().sum() == 0, "Answerì— null ê°’ ì¡´ì¬"
        
        # ë‹µë³€ ê¸¸ì´ ê²€ì¦
        long_answers = verify_df[verify_df['Answer'].str.len() > 500]
        if len(long_answers) > 0:
            print(f"   âš ï¸ ê¸´ ë‹µë³€ {len(long_answers)}ê°œ ë°œê²¬ (500ì ì´ˆê³¼)")
        
        print(f"   âœ… ì œì¶œíŒŒì¼ ê²€ì¦ ì™„ë£Œ")
        print(f"   - ì´ í–‰ ìˆ˜: {len(verify_df)}")
        print(f"   - ID ë²”ìœ„: {verify_df['ID'].min()} ~ {verify_df['ID'].max()}")
        print(f"   - ë‹µë³€ í‰ê·  ê¸¸ì´: {verify_df['Answer'].str.len().mean():.1f}ì")
        
    except Exception as e:
        print(f"   âŒ ì œì¶œíŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False
    
    print(f"\nğŸ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    return True


if __name__ == "__main__":
    success = generate_final_submission()
    if success:
        print("\nğŸ¯ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì„±ê³µ!")
    else:
        print("\nğŸ’¥ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì‹¤íŒ¨!")
        sys.exit(1)