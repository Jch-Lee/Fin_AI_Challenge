#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± - ìƒì„¸ ë¡œê·¸ ë²„ì „
8-bit ì–‘ìí™” + ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ + í•˜ì´ë¸Œë¦¬ë“œ RAG
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import logging
import time
from datetime import datetime
from scripts.simple_improved_prompt import SimpleImprovedPrompt
from scripts.improved_answer_extraction import ImprovedAnswerExtractor

# ìƒì„¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('final_submission_verbose.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

from packages.rag.rag_pipeline import RAGPipeline
from packages.rag.embeddings.kure_embedder import KUREEmbedder
from packages.llm.qwen_quantized import QuantizedQwenLLM

def validate_answer_length(answer: str, question_id: str, is_mc: bool) -> str:
    """ë‹µë³€ ê¸¸ì´ ê²€ì¦ ë° ìˆ˜ì •"""
    if is_mc:
        if len(answer) > 2:
            logger.warning(f"{question_id}: ê°ê´€ì‹ ë‹µë³€ì´ ë„ˆë¬´ ê¹€ ({len(answer)}ì) -> '1'ë¡œ ìˆ˜ì •")
            return "1"
        return answer
    else:
        if len(answer) > 500:
            logger.warning(f"{question_id}: ì£¼ê´€ì‹ ë‹µë³€ì´ ë„ˆë¬´ ê¹€ ({len(answer)}ì) -> 500ìë¡œ ìë¦„")
            return answer[:500]
        if len(answer) < 10:
            logger.warning(f"{question_id}: ì£¼ê´€ì‹ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ ({len(answer)}ì)")
        return answer

def generate_final_submission():
    """ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± - ìƒì„¸ ë¡œê·¸ ë²„ì „"""
    
    logger.info("="*80)
    logger.info("ğŸš€ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì‹œì‘ (ìƒì„¸ ë¡œê·¸ ë²„ì „)")
    logger.info("="*80)
    logger.info(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("ì„¤ì •: 8-bit ì–‘ìí™” + í•˜ì´ë¸Œë¦¬ë“œ RAG + ê°œì„ ëœ í”„ë¡¬í”„íŠ¸")
    
    start_time = time.time()
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    logger.info("ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test_df = pd.read_csv("data/competition/test.csv")
    total_questions = len(test_df)
    logger.info(f"   ì´ {total_questions}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
    
    # 2. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”...")
    
    # Embedder
    logger.info("   1. KURE Embedder ì´ˆê¸°í™”...")
    embedder = KUREEmbedder(
        model_name="nlpai-lab/KURE-v1",
        batch_size=32,
        show_progress=False
    )
    logger.info("      âœ… Embedder ë¡œë“œ ì™„ë£Œ")
    
    # RAG Pipeline
    logger.info("   2. í•˜ì´ë¸Œë¦¬ë“œ RAG Pipeline ì´ˆê¸°í™”...")
    rag_pipeline = RAGPipeline(
        embedder=embedder,
        retriever_type="hybrid",
        knowledge_base_path="data/rag/knowledge_base_fixed",
        enable_reranking=False,
        initial_retrieve_k=10,
        final_k=5
    )
    logger.info("      âœ… í•˜ì´ë¸Œë¦¬ë“œ RAG Pipeline ì´ˆê¸°í™” ì™„ë£Œ")
    
    # LLM (8-bit)
    logger.info("   3. Qwen2.5-7B-Instruct (8-bit) ì´ˆê¸°í™”...")
    llm_start = time.time()
    llm = QuantizedQwenLLM(
        model_id="Qwen/Qwen2.5-7B-Instruct",
        quantization_type="8bit",
        cache_dir="./models"
    )
    llm_init_time = time.time() - llm_start
    logger.info(f"      âœ… 8-bit LLM ë¡œë“œ ì™„ë£Œ ({llm_init_time:.1f}ì´ˆ)")
    
    memory_info = llm.get_memory_footprint()
    logger.info(f"      GPU ë©”ëª¨ë¦¬: {memory_info}")
    
    # í”„ë¡¬í”„íŠ¸ ë° ì¶”ì¶œê¸°
    prompt_generator = SimpleImprovedPrompt()
    extractor = ImprovedAnswerExtractor()
    
    # 3. ì „ì²´ ì§ˆë¬¸ ì²˜ë¦¬
    logger.info(f"ğŸ¤– ì „ì²´ {total_questions}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘...")
    logger.info("="*80)
    
    results = []
    failed_count = 0
    mc_count = 0
    oe_count = 0
    total_inference_time = 0
    
    for idx, row in test_df.iterrows():
        question_id = row['ID']
        question = row['Question']
        
        current_num = idx + 1
        elapsed = time.time() - start_time
        
        # ë§¤ ì§ˆë¬¸ë§ˆë‹¤ ìƒì„¸ ë¡œê·¸
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“‹ ì§ˆë¬¸ {current_num}/{total_questions} ({current_num/total_questions*100:.1f}%)")
        logger.info(f"ğŸ†” ID: {question_id}")
        logger.info(f"â“ ì§ˆë¬¸: {question[:80]}...")
        logger.info(f"â±ï¸ ê²½ê³¼ì‹œê°„: {elapsed/60:.1f}ë¶„")
        
        if current_num > 1:
            avg_time = elapsed / (current_num - 1)
            remaining_questions = total_questions - current_num
            eta = remaining_questions * avg_time
            eta_time = datetime.fromtimestamp(time.time() + eta)
            logger.info(f"ğŸ• ì˜ˆìƒì™„ë£Œ: {eta_time.strftime('%H:%M:%S')} (ì•½ {eta/60:.0f}ë¶„ í›„)")
        
        try:
            # RAG ê²€ìƒ‰
            logger.info("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ RAG ê²€ìƒ‰ ì¤‘...")
            search_start = time.time()
            
            contexts_list = rag_pipeline.retrieve(
                query=question,
                top_k=5,
                use_reranking=False
            )
            
            search_time = time.time() - search_start
            
            if contexts_list:
                contexts = [doc.get('content', '') for doc in contexts_list]
                logger.info(f"   âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(contexts_list)}ê°œ ë¬¸ì„œ ({search_time:.2f}ì´ˆ)")
                logger.info(f"   ğŸ“Š ì²« ë²ˆì§¸ ë¬¸ì„œ ì ìˆ˜: {contexts_list[0].get('score', 0):.4f}")
            else:
                contexts = []
                logger.info(f"   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ({search_time:.2f}ì´ˆ)")
            
            # ê°ê´€ì‹ ì—¬ë¶€ íŒë‹¨
            is_mc = any(f"{i}." in question or f"{i} " in question for i in range(1, 10))
            question_type = "ê°ê´€ì‹" if is_mc else "ì£¼ê´€ì‹"
            logger.info(f"ğŸ“ ì§ˆë¬¸ ìœ í˜•: {question_type}")
            
            if is_mc:
                mc_count += 1
            else:
                oe_count += 1
            
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            logger.info("ğŸ“ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            prompt = prompt_generator.create_improved_prompt(
                question=question,
                contexts=contexts,
                is_mc=is_mc
            )
            logger.info(f"   âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)}ì)")
            
            # LLM ìƒì„±
            logger.info("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
            inference_start = time.time()
            
            response = llm.generate_optimized(
                prompt=prompt,
                max_new_tokens=128 if is_mc else 256,
                temperature=0.1,
                use_cache=False
            )
            
            inference_time = time.time() - inference_start
            total_inference_time += inference_time
            
            logger.info(f"   âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ ({inference_time:.2f}ì´ˆ)")
            logger.info(f"   ğŸ“„ ìƒì„±ëœ ë‹µë³€: {response[:100]}...")
            
            # ê°œì„ ëœ ë‹µë³€ ì¶”ì¶œ
            if is_mc:
                max_choice = extractor.extract_choice_range(question)
                answer = extractor.extract_answer_from_response(
                    response=response,
                    max_choice=max_choice,
                    question=question
                )
                logger.info(f"   ğŸ¯ ì„ íƒì§€ ë²”ìœ„: 1-{max_choice}")
                logger.info(f"   âœ… ì¶”ì¶œëœ ë‹µë³€: '{answer}'")
            else:
                answer = extractor.extract_open_ended_answer(response)
                logger.info(f"   âœ… ì¶”ì¶œëœ ë‹µë³€: {answer[:100]}... (ì´ {len(answer)}ì)")
            
            # ë‹µë³€ ê¸¸ì´ ê²€ì¦
            validated_answer = validate_answer_length(answer, question_id, is_mc)
            if validated_answer != answer:
                logger.info(f"   ğŸ”§ ë‹µë³€ ìˆ˜ì •ë¨: '{answer}' â†’ '{validated_answer}'")
                answer = validated_answer
            
            total_time = search_time + inference_time
            logger.info(f"   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: {total_time:.2f}ì´ˆ (ê²€ìƒ‰: {search_time:.2f}ì´ˆ, ìƒì„±: {inference_time:.2f}ì´ˆ)")
            logger.info(f"   ğŸ‰ ìµœì¢… ë‹µë³€: '{answer}'")
            
        except Exception as e:
            logger.error(f"   âŒ ì§ˆë¬¸ {question_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            answer = "1" if is_mc else "ì²˜ë¦¬ ì‹¤íŒ¨"
            failed_count += 1
            logger.info(f"   ğŸ”§ ê¸°ë³¸ê°’ ì‚¬ìš©: '{answer}'")
        
        results.append({
            "ID": question_id,
            "Answer": answer
        })
        
        # í†µê³„ ìš”ì•½ (ë§¤ 25ê°œë§ˆë‹¤)
        if current_num % 25 == 0:
            logger.info(f"\nğŸ“Š ì¤‘ê°„ í†µê³„ (ì§ˆë¬¸ {current_num}/{total_questions}):")
            logger.info(f"   ê°ê´€ì‹: {mc_count}ê°œ, ì£¼ê´€ì‹: {oe_count}ê°œ, ì‹¤íŒ¨: {failed_count}ê°œ")
            avg_inference = total_inference_time / current_num
            logger.info(f"   í‰ê·  ì¶”ë¡ ì‹œê°„: {avg_inference:.2f}ì´ˆ/ì§ˆë¬¸")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import torch
            torch.cuda.empty_cache()
            logger.info("   ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    total_time = time.time() - start_time
    avg_inference_time = total_inference_time / total_questions
    
    # 4. ê²°ê³¼ ì €ì¥
    logger.info(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    submission_df = pd.DataFrame(results)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    submission_path = f"submission_8bit_verbose_{timestamp}.csv"
    
    submission_df.to_csv(submission_path, index=False, encoding='utf-8-sig')
    logger.info(f"   âœ… ì œì¶œíŒŒì¼ ì €ì¥: {submission_path}")
    
    backup_path = f"test_results/submission_8bit_verbose_backup_{timestamp}.csv"
    submission_df.to_csv(backup_path, index=False, encoding='utf-8-sig')
    logger.info(f"   âœ… ë°±ì—…íŒŒì¼ ì €ì¥: {backup_path}")
    
    # 5. ìµœì¢… í†µê³„
    logger.info(f"\n" + "="*80)
    logger.info("ğŸ‰ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì™„ë£Œ!")
    logger.info("="*80)
    
    logger.info(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    logger.info(f"   ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}ê°œ")
    logger.info(f"   ê°ê´€ì‹: {mc_count}ê°œ ({mc_count/total_questions*100:.1f}%)")
    logger.info(f"   ì£¼ê´€ì‹: {oe_count}ê°œ ({oe_count/total_questions*100:.1f}%)")
    logger.info(f"   ì²˜ë¦¬ ì‹¤íŒ¨: {failed_count}ê°œ ({failed_count/total_questions*100:.1f}%)")
    
    logger.info(f"\nâ±ï¸ ì‹œê°„ í†µê³„:")
    logger.info(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/60:.1f}ë¶„")
    logger.info(f"   LLM ì´ˆê¸°í™”: {llm_init_time:.1f}ì´ˆ")
    logger.info(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.2f}ì´ˆ/ì§ˆë¬¸")
    logger.info(f"   ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time/60:.1f}ë¶„")
    
    logger.info(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:")
    final_memory = llm.get_memory_footprint()
    logger.info(f"   ìµœì¢… GPU ë©”ëª¨ë¦¬: {final_memory}")
    logger.info(f"   ì–‘ìí™” ë°©ì‹: 8-bit")
    logger.info(f"   RAG ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (BM25 + Vector)")
    
    logger.info(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    logger.info(f"   ë©”ì¸ ì œì¶œíŒŒì¼: {submission_path}")
    logger.info(f"   ë°±ì—… íŒŒì¼: {backup_path}")
    
    # 6. íŒŒì¼ ê²€ì¦
    logger.info(f"\nğŸ” ì œì¶œíŒŒì¼ ê²€ì¦...")
    try:
        verify_df = pd.read_csv(submission_path)
        
        assert len(verify_df) == total_questions, f"í–‰ ìˆ˜ ë¶ˆì¼ì¹˜: {len(verify_df)} != {total_questions}"
        assert list(verify_df.columns) == ['ID', 'Answer'], f"ì»¬ëŸ¼ëª… ì˜¤ë¥˜: {list(verify_df.columns)}"
        assert verify_df['ID'].isnull().sum() == 0, "IDì— null ê°’ ì¡´ì¬"
        assert verify_df['Answer'].isnull().sum() == 0, "Answerì— null ê°’ ì¡´ì¬"
        
        long_answers = verify_df[verify_df['Answer'].str.len() > 500]
        if len(long_answers) > 0:
            logger.warning(f"   âš ï¸ ê¸´ ë‹µë³€ {len(long_answers)}ê°œ ë°œê²¬ (500ì ì´ˆê³¼)")
        
        logger.info(f"   âœ… ì œì¶œíŒŒì¼ ê²€ì¦ ì™„ë£Œ")
        logger.info(f"   - ì´ í–‰ ìˆ˜: {len(verify_df)}")
        logger.info(f"   - ID ë²”ìœ„: {verify_df['ID'].min()} ~ {verify_df['ID'].max()}")
        logger.info(f"   - ë‹µë³€ í‰ê·  ê¸¸ì´: {verify_df['Answer'].str.len().mean():.1f}ì")
        
        # ìƒ˜í”Œ ë‹µë³€ ì¶œë ¥
        logger.info(f"\nğŸ” ìƒ˜í”Œ ë‹µë³€ (ì²˜ìŒ 10ê°œ):")
        for i in range(min(10, len(verify_df))):
            row = verify_df.iloc[i]
            logger.info(f"   {row['ID']}: {row['Answer'][:50]}...")
        
    except Exception as e:
        logger.error(f"   âŒ ì œì¶œíŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False
    
    logger.info(f"\nğŸ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    return True


if __name__ == "__main__":
    success = generate_final_submission()
    if success:
        logger.info("\nğŸ¯ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì„±ê³µ!")
    else:
        logger.error("\nğŸ’¥ ìµœì¢… ì œì¶œíŒŒì¼ ìƒì„± ì‹¤íŒ¨!")
        sys.exit(1)