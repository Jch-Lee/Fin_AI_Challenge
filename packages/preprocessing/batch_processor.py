"""
Î∞∞Ïπò PDF Ï≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏
- Vision V2 Í∏∞Î∞ò PDF ÌååÏã±
- Kiwi Í∏∞Î∞ò ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ (ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
- Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ ÎåÄÏö©Îüâ Ï≤òÎ¶¨
"""

import os
import gc
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import torch

logger = logging.getLogger(__name__)

# Ï°∞Í±¥Î∂Ä imports
from . import TextProcessor, KIWI_AVAILABLE, LANGCHAIN_AVAILABLE
from .chunker import DocumentChunker
if LANGCHAIN_AVAILABLE:
    from .hierarchical_chunker import HierarchicalMarkdownChunker
from .embedder import EmbeddingGenerator
from ..rag.knowledge_base import KnowledgeBase

# Vision ÌååÏÑú import
try:
    from ..parsers.vision_v2_parser import VisionV2Parser
    VISION_AVAILABLE = True
except ImportError:
    logger.warning("Vision V2 parser not available")
    VISION_AVAILABLE = False
    from ..parsers.pdf_parser import PyMuPDFParser as VisionV2Parser


@dataclass
class ProcessingResult:
    """Ï≤òÎ¶¨ Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§"""
    file_path: str
    status: str  # 'success', 'partial', 'failed'
    chunks_created: int
    embeddings_created: int
    error_message: Optional[str] = None
    processing_time: float = 0.0


class BatchPDFProcessor:
    """
    ÎåÄÍ∑úÎ™® PDF Î∞∞Ïπò Ï≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏
    - data/rawÏùò PDFÎ•º Î≤°ÌÑ∞ DBÎ°ú Î≥ÄÌôò
    - Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ Ï†êÏßÑÏ†Å Ï≤òÎ¶¨
    """
    
    def __init__(self, 
                 use_kiwi: bool = True,
                 chunk_size: int = 512,
                 chunk_overlap: int = 50,
                 batch_size: int = 10,
                 checkpoint_interval: int = 100):
        """
        Args:
            use_kiwi: Kiwi ÌÖçÏä§Ìä∏ ÌîÑÎ°úÏÑ∏ÏÑú ÏÇ¨Ïö© Ïó¨Î∂Ä
            chunk_size: Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞
            chunk_overlap: Ï≤≠ÌÅ¨ Ïò§Î≤ÑÎû©
            batch_size: Ìïú Î≤àÏóê Ï≤òÎ¶¨Ìï† PDF Ïàò
            checkpoint_interval: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû• Í∞ÑÍ≤©
        """
        # Vision ÌååÏÑú Ï¥àÍ∏∞Ìôî
        self.vision_parser = VisionV2Parser()
        
        # ÌÖçÏä§Ìä∏ ÌîÑÎ°úÏÑ∏ÏÑú ÏÑ†ÌÉù
        if use_kiwi and KIWI_AVAILABLE:
            logger.info("Using KiwiTextProcessor for text processing")
            self.text_processor = TextProcessor(
                num_workers=4,  # Î≥ëÎ†¨ Ï≤òÎ¶¨
                apply_spacing=True,
                apply_normalization=True
            )
            self.using_kiwi = True
        else:
            if use_kiwi and not KIWI_AVAILABLE:
                logger.warning("Kiwi requested but not available, using basic processor")
            logger.info("Using basic KoreanEnglishTextProcessor")
            self.text_processor = TextProcessor()
            self.using_kiwi = False
        
        # Ï≤≠ÌÇπ Î∞è ÏûÑÎ≤†Îî©
        # Vision V2Îäî ÎßàÌÅ¨Îã§Ïö¥ÏùÑ ÏÉùÏÑ±ÌïòÎØÄÎ°ú Í≥ÑÏ∏µÏ†Å Ï≤≠ÌÇπ Ïö∞ÏÑ† ÏÇ¨Ïö©
        if LANGCHAIN_AVAILABLE and VISION_AVAILABLE:
            logger.info("Using HierarchicalMarkdownChunker for Vision V2 markdown output")
            self.chunker = HierarchicalMarkdownChunker(
                use_chunk_cleaner=True,
                enable_semantic=False
            )
        else:
            logger.info(f"Using basic DocumentChunker (langchain: {LANGCHAIN_AVAILABLE}, vision: {VISION_AVAILABLE})")
            self.chunker = DocumentChunker(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
        self.embedder = EmbeddingGenerator()
        
        # ÏßÄÏãù Î≤†Ïù¥Ïä§
        self.knowledge_base = KnowledgeBase()
        
        # Î∞∞Ïπò Ï≤òÎ¶¨ ÏÑ§Ï†ï
        self.batch_size = batch_size
        self.checkpoint_interval = checkpoint_interval
        
        # Ï≤òÎ¶¨ ÌÜµÍ≥Ñ
        self.stats = {
            'total_files': 0,
            'success': 0,
            'partial': 0,
            'failed': 0,
            'total_chunks': 0,
            'total_embeddings': 0
        }
    
    def process_pdf_directory(self, 
                            pdf_dir: str = "data/raw",
                            output_dir: str = "data/processed",
                            resume_from: Optional[str] = None) -> List[ProcessingResult]:
        """
        ÎîîÎ†âÌÜ†Î¶¨Ïùò Î™®Îì† PDF Ï≤òÎ¶¨
        
        Args:
            pdf_dir: PDF ÌååÏùº ÎîîÎ†âÌÜ†Î¶¨
            output_dir: Ï≤òÎ¶¨ Í≤∞Í≥º Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
            resume_from: Ïû¨Í∞úÌï† ÌååÏùº Í≤ΩÎ°ú (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏)
        """
        # ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏
        pdf_path = Path(pdf_dir)
        if not pdf_path.exists():
            raise ValueError(f"PDF directory not found: {pdf_dir}")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # PDF ÌååÏùº Î™©Î°ù
        pdf_files = sorted(pdf_path.glob("*.pdf"))
        if not pdf_files:
            logger.warning(f"No PDF files found in {pdf_dir}")
            return []
        
        logger.info(f"Found {len(pdf_files)} PDF files to process")
        
        # Ïû¨Í∞ú ÏßÄÏ†ê Ï∞æÍ∏∞
        start_idx = 0
        if resume_from:
            for i, pdf_file in enumerate(pdf_files):
                if str(pdf_file) == resume_from:
                    start_idx = i + 1
                    logger.info(f"Resuming from file {start_idx}/{len(pdf_files)}")
                    break
        
        # Î∞∞Ïπò Ï≤òÎ¶¨
        results = []
        for batch_start in range(start_idx, len(pdf_files), self.batch_size):
            batch_end = min(batch_start + self.batch_size, len(pdf_files))
            batch_files = pdf_files[batch_start:batch_end]
            
            logger.info(f"Processing batch {batch_start+1}-{batch_end}/{len(pdf_files)}")
            
            # Î∞∞Ïπò Ï≤òÎ¶¨
            batch_results = self._process_batch(batch_files)
            results.extend(batch_results)
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
            if (batch_end % self.checkpoint_interval) == 0:
                self._save_checkpoint(output_dir, batch_end, results)
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            self._cleanup_memory()
        
        # ÏµúÏ¢Ö Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏
        self._save_checkpoint(output_dir, len(pdf_files), results)
        
        # ÌÜµÍ≥Ñ Ï∂úÎ†•
        self._print_stats()
        
        return results
    
    def process_single_pdf(self, pdf_path: str) -> ProcessingResult:
        """
        Îã®Ïùº PDF ÌååÏùº Ï≤òÎ¶¨
        
        Args:
            pdf_path: PDF ÌååÏùº Í≤ΩÎ°ú
        """
        import time
        start_time = time.time()
        
        try:
            # 1. PDF ÌååÏã± (Vision V2)
            logger.info(f"Parsing PDF: {pdf_path}")
            parsed_content = self.vision_parser.parse_pdf(pdf_path)
            
            if not parsed_content or not parsed_content.get('text'):
                raise ValueError("No text extracted from PDF")
            
            # 2. ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
            logger.info("Processing text...")
            processed_text = self.text_processor.process(
                parsed_content['text'],
                preserve_paragraphs=True,
                financial_processing=True
            )
            
            # 3. Ï≤≠ÌÇπ
            logger.info("Creating chunks...")
            chunks = self.chunker.chunk_text(processed_text)
            
            if not chunks:
                raise ValueError("No chunks created from text")
            
            # 4. ÏûÑÎ≤†Îî© ÏÉùÏÑ±
            logger.info(f"Generating embeddings for {len(chunks)} chunks...")
            embeddings = self.embedder.embed_batch(chunks)
            
            # 5. ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê Ï∂îÍ∞Ä
            doc_ids = [f"{Path(pdf_path).stem}_chunk_{i}" for i in range(len(chunks))]
            self.knowledge_base.add_documents(chunks, embeddings, doc_ids)
            
            # ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self.stats['success'] += 1
            self.stats['total_chunks'] += len(chunks)
            self.stats['total_embeddings'] += len(embeddings)
            
            processing_time = time.time() - start_time
            
            return ProcessingResult(
                file_path=pdf_path,
                status='success',
                chunks_created=len(chunks),
                embeddings_created=len(embeddings),
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Failed to process {pdf_path}: {e}")
            self.stats['failed'] += 1
            
            processing_time = time.time() - start_time
            
            return ProcessingResult(
                file_path=pdf_path,
                status='failed',
                chunks_created=0,
                embeddings_created=0,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def _process_batch(self, pdf_files: List[Path]) -> List[ProcessingResult]:
        """Î∞∞Ïπò Ï≤òÎ¶¨"""
        results = []
        
        for pdf_file in pdf_files:
            self.stats['total_files'] += 1
            result = self.process_single_pdf(str(pdf_file))
            results.append(result)
            
            # ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†•
            if result.status == 'success':
                logger.info(f"‚úÖ Processed: {pdf_file.name} "
                          f"({result.chunks_created} chunks, "
                          f"{result.processing_time:.1f}s)")
            else:
                logger.error(f"‚ùå Failed: {pdf_file.name} - {result.error_message}")
        
        return results
    
    def _cleanup_memory(self):
        """Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Vision Î™®Îç∏ Ï£ºÍ∏∞Ï†Å Ïñ∏Î°úÎìú (Î©îÎ™®Î¶¨ Ï†àÏïΩ)
        if hasattr(self.vision_parser, 'unload_model'):
            if self.stats['total_files'] % 20 == 0:
                logger.info("Unloading Vision model to free memory...")
                self.vision_parser.unload_model()
    
    def _save_checkpoint(self, output_dir: str, processed_count: int, results: List[ProcessingResult]):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•"""
        import json
        
        checkpoint_path = Path(output_dir) / f"checkpoint_{processed_count}.json"
        
        checkpoint_data = {
            'processed_count': processed_count,
            'stats': self.stats,
            'using_kiwi': self.using_kiwi,
            'results': [
                {
                    'file_path': r.file_path,
                    'status': r.status,
                    'chunks_created': r.chunks_created,
                    'embeddings_created': r.embeddings_created,
                    'error_message': r.error_message,
                    'processing_time': r.processing_time
                }
                for r in results
            ]
        }
        
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Checkpoint saved: {checkpoint_path}")
        
        # ÏßÄÏãù Î≤†Ïù¥Ïä§ Ï†ÄÏû•
        if hasattr(self.knowledge_base, 'save'):
            kb_path = Path(output_dir) / f"knowledge_base_{processed_count}.pkl"
            self.knowledge_base.save(str(kb_path))
    
    def _print_stats(self):
        """ÌÜµÍ≥Ñ Ï∂úÎ†•"""
        logger.info("\n" + "="*50)
        logger.info("üìä Processing Statistics")
        logger.info("="*50)
        logger.info(f"Total Files: {self.stats['total_files']}")
        logger.info(f"Success: {self.stats['success']}")
        logger.info(f"Failed: {self.stats['failed']}")
        logger.info(f"Total Chunks: {self.stats['total_chunks']}")
        logger.info(f"Total Embeddings: {self.stats['total_embeddings']}")
        
        if self.stats['total_files'] > 0:
            success_rate = (self.stats['success'] / self.stats['total_files']) * 100
            avg_chunks = self.stats['total_chunks'] / max(self.stats['success'], 1)
            logger.info(f"Success Rate: {success_rate:.1f}%")
            logger.info(f"Avg Chunks per File: {avg_chunks:.1f}")
        
        logger.info(f"Text Processor: {'Kiwi' if self.using_kiwi else 'Basic'}")
        logger.info("="*50)


# Ïä§ÌÅ¨Î¶ΩÌä∏Î°ú ÏßÅÏ†ë Ïã§Ìñâ
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Batch PDF Processing Pipeline")
    parser.add_argument("--pdf-dir", default="data/raw", help="PDF directory")
    parser.add_argument("--output-dir", default="data/processed", help="Output directory")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size")
    parser.add_argument("--no-kiwi", action="store_true", help="Disable Kiwi processor")
    parser.add_argument("--resume", help="Resume from checkpoint file")
    
    args = parser.parse_args()
    
    # ÌîÑÎ°úÏÑ∏ÏÑú Ï¥àÍ∏∞Ìôî
    processor = BatchPDFProcessor(
        use_kiwi=not args.no_kiwi,
        batch_size=args.batch_size
    )
    
    # Ï≤òÎ¶¨ Ïã§Ìñâ
    results = processor.process_pdf_directory(
        pdf_dir=args.pdf_dir,
        output_dir=args.output_dir,
        resume_from=args.resume
    )
    
    print(f"\n‚úÖ Processing complete: {len(results)} files processed")