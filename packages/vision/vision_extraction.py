"""
Vision-Language ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“ˆ
Qwen2.5-VL-7B-Instruct ëª¨ë¸ì„ ì‚¬ìš©í•œ PDF ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
"""

import os
import io
import torch
import logging
import re
from PIL import Image
from typing import Optional, Dict, Any
import pymupdf

logger = logging.getLogger(__name__)


class VisionTextExtractor:
    """Vision-Language ëª¨ë¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self, device: Optional[str] = None):
        """
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', ë˜ëŠ” None for auto)
        """
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.processor = None
        self._initialized = False
        
        # ì¶”ê°€: í˜ì´ì§€ ê°„ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
        self.last_page_info = {
            'last_header': None,
            'last_sentence': None,
            'incomplete': False
        }
        
        logger.info(f"VisionTextExtractor initialized with device: {self.device}")
    
    def _ensure_initialized(self):
        """ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”"""
        if not self._initialized:
            self._initialize_model()
    
    def _initialize_model(self):
        """Vision ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
            
            logger.info("Loading Qwen2.5-VL-7B-Instruct model...")
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "Qwen/Qwen2.5-VL-7B-Instruct",
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                "Qwen/Qwen2.5-VL-7B-Instruct",
                trust_remote_code=True
            )
            
            if self.device == "cpu":
                self.model = self.model.cpu()
            
            self._initialized = True
            logger.info("Vision model initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize vision model: {e}")
            raise
    
    def _get_extraction_prompt(self) -> str:
        """ì¶•ì•½ëœ Version 2.5 í”„ë¡¬í”„íŠ¸ (70ì¤„)"""
        return """ì´ ë¬¸ì„œ í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ìš°ì„ ìˆœìœ„ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

**1ìˆœìœ„: í•µì‹¬ í…ìŠ¤íŠ¸**
- ì œëª©, ì†Œì œëª©, ë³¸ë¬¸, ëª©ë¡ êµ¬ì¡°, ê°ì£¼

**2ìˆœìœ„: í‘œì™€ ë°ì´í„°**
- í‘œ ì œëª©/í—¤ë”, ë°ì´í„° ê°’ê³¼ ë‹¨ìœ„, í–‰/ì—´ ê´€ê³„

**3ìˆœìœ„: ì°¨íŠ¸/ê·¸ë˜í”„**
- ì œëª©, ì¶• ë ˆì´ë¸”, ë°ì´í„° í¬ì¸íŠ¸, ë²”ë¡€, ìˆ˜ì¹˜

**4ìˆœìœ„: êµ¬ì¡°ì  ìš”ì†Œ**
- ì„¹ì…˜ êµ¬ë¶„, í•˜ì´ë¼ì´íŠ¸, ë‹¤ì´ì–´ê·¸ë¨ í…ìŠ¤íŠ¸

**ì œì™¸ì‚¬í•­:**
- ìƒ‰ìƒ/í°íŠ¸/ìœ„ì¹˜ ë“± ìŠ¤íƒ€ì¼, ë ˆì´ì•„ì›ƒ/ë””ìì¸, ì¥ì‹ ìš”ì†Œ

ì‹¤ì œ ì •ë³´ì™€ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ì •ë¦¬í•˜ì„¸ìš”.

---

## ë§ˆí¬ë‹¤ìš´ í˜•ì‹í™”

### í—¤ë”
- ì£¼ì œëª©: #
- ì¤‘ì œëª©: ##  
- ì†Œì œëª©: ###
- ì„¸ë¶€: ####

### êµ¬ì¡°
- ë¬¸ë‹¨: ë¹ˆ ì¤„ë¡œ êµ¬ë¶„
- ë²ˆí˜¸ ëª©ë¡: 1. 2. 3.
- ê¸€ë¨¸ë¦¬: - ë˜ëŠ” *
- ë“¤ì—¬ì“°ê¸°: 2ì¹¸ ë˜ëŠ” 4ì¹¸

### í‘œ
| í—¤ë”1 | í—¤ë”2 | í—¤ë”3 |
|-------|-------|-------|
| ë°ì´í„° | ë°ì´í„° | ë°ì´í„° |

### í˜ì´ì§€ ê²½ê³„
- ë¯¸ì™„ì„±: ... [ë‹¤ìŒ í˜ì´ì§€ì— ê³„ì†]
- ì—°ì†: [ì´ì „ í˜ì´ì§€ì—ì„œ ê³„ì†] ...

### íŠ¹ìˆ˜ ì½˜í…ì¸ 
- ì½”ë“œ: `ë°±í‹±`
- ê°•ì¡°: **êµµê²Œ** ë˜ëŠ” *ê¸°ìš¸ì„*
- ì¸ìš©: > ì‚¬ìš©

êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¡œ ìƒì„±í•˜ì„¸ìš”.

**ì¤‘ìš”**: ì´ í˜ì´ì§€ì˜ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ì¶œí•  ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ë‚´ìš© ì—†ìŒ"ë§Œ ì¶œë ¥."""
    
    def _get_contextual_prompt(self, page_num: int = 0) -> str:
        """í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_prompt = self._get_extraction_prompt()  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        
        # ì²« í˜ì´ì§€ê°€ ì•„ë‹ˆê³  ì´ì „ í˜ì´ì§€ê°€ ë¶ˆì™„ì „í•œ ê²½ìš°
        if page_num > 0 and self.last_page_info['incomplete']:
            context_hint = f"""

### ğŸ“Œ ì´ì „ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸
- ë§ˆì§€ë§‰ í—¤ë”: {self.last_page_info.get('last_header', 'ì—†ìŒ')}
- ë§ˆì§€ë§‰ ë¬¸ì¥ ì¼ë¶€: ...{self.last_page_info.get('last_sentence', '')[-50:]}

ì´ í˜ì´ì§€ê°€ ì´ì „ ë‚´ìš©ì˜ ì—°ì†ì´ë¼ë©´ [ì´ì „ í˜ì´ì§€ì—ì„œ ê³„ì†] í‘œì‹œë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.
"""
            return base_prompt + context_hint
        
        return base_prompt
    
    def _update_page_context(self, extracted_text: str):
        """í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
        
        lines = extracted_text.strip().split('\n')
        
        # ë§ˆì§€ë§‰ í—¤ë” ì°¾ê¸°
        headers = [line for line in lines if re.match(r'^#{1,4}\s', line)]
        if headers:
            self.last_page_info['last_header'] = headers[-1]
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì €ì¥
        if lines:
            last_line = lines[-1].strip()
            self.last_page_info['last_sentence'] = last_line
            
            # ë¶ˆì™„ì „ ì—¬ë¶€ íŒë‹¨
            self.last_page_info['incomplete'] = (
                '[ë‹¤ìŒ í˜ì´ì§€ì— ê³„ì†]' in last_line or
                not last_line.endswith(('.', '!', '?', 'ë‹¤.', 'ìš”.', 'ë‹ˆë‹¤.'))
            )
    
    def _filter_prompt_leakage(self, text: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ëˆ„ì¶œ ì œê±° ë° ë¹ˆ ë‚´ìš© ì²˜ë¦¬"""
        
        # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ë§ˆì»¤ ì‚¬ì´ì˜ ë‚´ìš© ì œê±°
        start_marker = "<!--PROMPT_START_MARKER_DO_NOT_INCLUDE-->"
        end_marker = "<!--PROMPT_END_MARKER_DO_NOT_INCLUDE-->"
        
        # ë§ˆì»¤ê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
        if start_marker in text:
            if end_marker in text:
                # ì‹œì‘ê³¼ ë ë§ˆì»¤ ì‚¬ì´ ëª¨ë“  ë‚´ìš© ì œê±°
                parts = text.split(start_marker)
                if len(parts) > 1:
                    remaining = parts[1].split(end_marker)
                    if len(remaining) > 1:
                        text = parts[0] + remaining[1]
                    else:
                        text = parts[0]
            else:
                # ì‹œì‘ ë§ˆì»¤ ì´í›„ ëª¨ë“  ë‚´ìš© ì œê±°
                text = text.split(start_marker)[0]
        
        # 2ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œ íŒ¨í„´ ì œê±°
        prompt_patterns = [
            r'ì´ ë¬¸ì„œ í˜ì´ì§€ì—ì„œ.*?ì¶”ì¶œí•´ì£¼ì„¸ìš”[.:]*',
            r'\*\*\d+ìˆœìœ„:.*?\*\*',
            r'ìœ„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼.*?ì •ë¦¬í•´ì£¼ì„¸ìš”[.:]*',
            r'ì¶”ê°€ ì§€ì¹¨:.*?ë§ˆí¬ë‹¤ìš´ í˜•ì‹í™”',
            r'í—¤ë” ë ˆë²¨ ì§€ì •',
            r'ë¬¸ë‹¨ê³¼ êµ¬ì¡°',
            r'í‘œ í˜•ì‹',
            r'í˜ì´ì§€ ê²½ê³„ í‘œì‹œ',
            r'íŠ¹ìˆ˜ ì½˜í…ì¸ ',
            r'ì´ëŸ¬í•œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„.*?ìƒì„±í•´ì£¼ì„¸ìš”[.:]*',
            r'\*\*ì¤‘ìš”\*\*:.*?ì¶œë ¥í•˜ì„¸ìš”[.:]*'
        ]
        
        for pattern in prompt_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # 3ë‹¨ê³„: ë¶ˆì™„ì „í•œ ë§ˆì»¤ë‚˜ ì§€ì¹¨ ì œê±°
        instruction_patterns = [
            r'###\s*(í—¤ë”|ë¬¸ë‹¨|í‘œ|í˜ì´ì§€|íŠ¹ìˆ˜).*?\n',
            r'-\s*[ê°€-í£\s]*:\s*[#`\*].*?\n',
            r'\|\s*í—¤ë”\d+\s*\|.*?\n',
            r'```\s*\n.*?\n```',
            r'ìœ„ì˜?\s*ì§€ì¹¨',
            r'ë‹¤ìŒ\s*ê·œì¹™',
            r'í˜•ì‹ìœ¼ë¡œ\s*ë³€í™˜'
        ]
        
        for pattern in instruction_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # 4ë‹¨ê³„: ì •ë¦¬ ë° ê²€ì¦
        text = text.strip()
        
        # ë¹ˆ ë‚´ìš©ì´ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ì¸ ê²½ìš°
        if not text or len(text) < 10:
            return "ë‚´ìš© ì—†ìŒ"
        
        # ì—¬ì „íˆ í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œê°€ ë§ì´ ë‚¨ì•„ìˆëŠ” ê²½ìš°
        prompt_keywords = ['ìš°ì„ ìˆœìœ„', 'ì¶”ì¶œ', 'ì§€ì¹¨', 'í˜•ì‹', 'ë³€í™˜', 'ìƒì„±í•´ì£¼ì„¸ìš”']
        keyword_count = sum(1 for keyword in prompt_keywords if keyword in text)
        
        if keyword_count >= 3 and len(text) < 200:
            return "ë‚´ìš© ì—†ìŒ"
        
        # 5ë‹¨ê³„: ìµœì¢… ì •ë¦¬
        # ì—°ì†ëœ ë¹ˆ ì¤„ ì œê±°
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ê° ì¤„ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(line for line in lines if line)
        
        return text.strip() if text.strip() else "ë‚´ìš© ì—†ìŒ"
    
    def pdf_page_to_image(self, pdf_path: str, page_num: int, dpi: int = 150) -> Image.Image:
        """PDF í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            doc = pymupdf.open(pdf_path)
            page = doc[page_num]
            pix = page.get_pixmap(dpi=dpi)
            img_data = pix.tobytes("png")
            doc.close()
            return Image.open(io.BytesIO(img_data))
        except Exception as e:
            logger.error(f"Failed to convert PDF page {page_num + 1} to image: {e}")
            raise
    
    def extract_text_from_image(self, image: Image.Image, page_num: int = 0) -> str:
        """ì´ë¯¸ì§€ì—ì„œ VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        self._ensure_initialized()
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = self._get_contextual_prompt(page_num)
            
            # qwen_vl_utils ì‚¬ìš© ì‹œë„
            try:
                from qwen_vl_utils import process_vision_info
                use_utils = True
            except ImportError:
                process_vision_info = None
                use_utils = False
                logger.debug("qwen_vl_utils not available, using fallback")
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            if use_utils and process_vision_info:
                image_inputs, video_inputs = process_vision_info(messages)
            else:
                image_inputs = [image]
                video_inputs = []
            
            # ì…ë ¥ ì²˜ë¦¬
            inputs = self.processor(
                text=[text],
                images=image_inputs if image_inputs else None,
                videos=video_inputs if video_inputs else None,
                padding=True,
                return_tensors="pt"
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.device == "cuda":
                inputs = {k: v.to("cuda") if hasattr(v, 'to') else v 
                         for k, v in inputs.items()}
            
            # ìƒì„± ì„¤ì •
            generation_config = {
                "max_new_tokens": 2048,
                "temperature": 0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
                "do_sample": True,
                "top_p": 0.9,
                "pad_token_id": self.processor.tokenizer.eos_token_id
            }
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_config)
            
            # ì…ë ¥ ê¸¸ì´ ê³„ì‚° ë° ì¶œë ¥ ë””ì½”ë”©
            input_ids_tensor = inputs.get('input_ids')
            if input_ids_tensor is not None and len(outputs) > 0:
                input_len = input_ids_tensor.shape[1]
                generated_ids = []
                
                for output_ids in outputs:
                    if len(output_ids) > input_len:
                        generated_ids.append(output_ids[input_len:])
                    else:
                        generated_ids.append(output_ids)
                
                if generated_ids:
                    decoded_outputs = self.processor.batch_decode(
                        generated_ids,
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=False
                    )
                    
                    if decoded_outputs:
                        result_text = decoded_outputs[0].strip()
                        # í”„ë¡¬í”„íŠ¸ ëˆ„ì¶œ í•„í„°ë§
                        filtered_text = self._filter_prompt_leakage(result_text)
                        # ì¶”ì¶œ ê²°ê³¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                        self._update_page_context(filtered_text)
                        return filtered_text
            
            return "[í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨]"
            
        except Exception as e:
            logger.error(f"Vision text extraction failed: {e}")
            return f"[VL ëª¨ë¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}]"
    
    def extract_pdf_page(self, pdf_path: str, page_num: int) -> Dict[str, Any]:
        """PDFì˜ íŠ¹ì • í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # PDF í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            image = self.pdf_page_to_image(pdf_path, page_num)
            
            # VL ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í˜ì´ì§€ ë²ˆí˜¸ ì „ë‹¬)
            extracted_text = self.extract_text_from_image(image, page_num)
            
            # ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ê²€ì¦
            structure_quality = self._validate_markdown_structure(extracted_text)
            
            return {
                "status": "success",
                "page_num": page_num + 1,
                "text": extracted_text,
                "char_count": len(extracted_text),
                "extraction_method": "vision_v2.5",
                "markdown_quality": structure_quality
            }
            
        except Exception as e:
            logger.error(f"Failed to extract text from page {page_num + 1}: {e}")
            return {
                "status": "error",
                "page_num": page_num + 1,
                "text": f"[í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}]",
                "char_count": 0,
                "extraction_method": "vision_v2.5",
                "error": str(e)
            }
    
    def is_available(self) -> bool:
        """Vision ì¶”ì¶œì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
        try:
            # GPU ì²´í¬
            if not torch.cuda.is_available():
                logger.info("CUDA not available for vision extraction")
                return False
            
            # ëª¨ë¸ import ì²´í¬
            from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
            return True
            
        except ImportError as e:
            logger.warning(f"Vision model dependencies not available: {e}")
            return False
        except Exception as e:
            logger.error(f"Vision extraction availability check failed: {e}")
            return False
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if self.processor is not None:
            del self.processor
            self.processor = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self._initialized = False
        logger.info("Vision model cleaned up")
    
    def _validate_markdown_structure(self, text: str) -> Dict[str, Any]:
        """ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° í’ˆì§ˆ ê²€ì¦"""
        
        return {
            'has_headers': bool(re.search(r'^#{1,4}\s', text, re.MULTILINE)),
            'has_lists': bool(re.search(r'^[\-\*\d]+\.?\s', text, re.MULTILINE)),
            'has_tables': '|' in text and '---' in text,
            'has_continuity_markers': '[ê³„ì†]' in text or '[ì´ì–´ì„œ]' in text,
            'header_count': len(re.findall(r'^#{1,4}\s', text, re.MULTILINE)),
            'paragraph_count': len([line for line in text.split('\n\n') if line.strip()]),
            'avg_paragraph_length': sum(len(p) for p in text.split('\n\n') if p.strip()) / max(len([p for p in text.split('\n\n') if p.strip()]), 1),
            'structure_score': self._calculate_structure_score(text)
        }
    
    def _calculate_structure_score(self, text: str) -> float:
        """êµ¬ì¡°í™” ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        
        score = 0.0
        
        # í—¤ë” ì¡´ì¬ (0.3)
        if re.search(r'^#{1,4}\s', text, re.MULTILINE):
            score += 0.3
        
        # ë¦¬ìŠ¤íŠ¸ ì¡´ì¬ (0.2)
        if re.search(r'^[\-\*\d]+\.?\s', text, re.MULTILINE):
            score += 0.2
        
        # ì ì ˆí•œ ë¬¸ë‹¨ êµ¬ë¶„ (0.2)
        if '\n\n' in text:
            score += 0.2
        
        # í˜ì´ì§€ ì—°ì†ì„± ë§ˆì»¤ (0.1)
        if '[ê³„ì†]' in text or '[ì´ì–´ì„œ]' in text:
            score += 0.1
        
        # í‘œ í˜•ì‹ (0.1)
        if '|' in text and '---' in text:
            score += 0.1
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë³´ì • (0.1)
        if len(text.strip()) > 100:
            score += 0.1
        
        return min(score, 1.0)