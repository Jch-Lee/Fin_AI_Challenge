### DP - SGD ( Differential Privacy - Stochastic Gradient Descent ) 기법 - SGD 는 AI 모델 을 학습 * 방식 의 대표 적 이 ᆫ 방법 으로 입력 데이터 를 작 은 크기 로 분할 집합 ( Mini Batch ) 하 어 학습 진행 * AI 모델학습 은 새롭 은 데이터 입력 으로 AI 모델 내 의 노드 등 의 수치 가 최적화 ( Optimize ) 되 는 과정 으로 해석 - DP - SGD 는 SGD 방식 에서 차등 프라이버시 기법 을 적용 하 어 학습 진행 - 분할 집합 마다 각 가중 치 를 구하 고 최대 기울기 제한 ( Clip Gradient ) 및 통계 적 기반 의 노이즈 ( Gaussian Noise ) 를 추가 하 어 학습 진행 - 해당 기법 으로 공격자 는 출력 값 을 기반 으로 입력 값 을 유추 , 모델 유추 등 이 어렵 음 ### 목표 함수 노이즈 ( 27 ) ) - 목표 함수 ( Object Function ) * 에차 등 프라이버시 기법 적용 을 통하 어 , 입력 값 을 유추 하 ᆯ 수 없 도록 하 ᆫ다 . * 학습 과정 에서 입력 데이터 에서 의 실제 정답 ( 레이블 ) 과 AI 모델 의 예측 값 사이 의 차이 ( 손실 값 ) 를 판단 하 는 수식 함수 ### 데이터 분할 방식 ( 28 ) ) - 민감 데이터 를 분할 하 고 각 민감 데이터 마다 각기 다른 분류 기 로 학습 및 각 분류 기 를 조합 하 어 학습 하 는 방식 을 제안 하 었 다 . - 이 방식 을 통하 어 , 출력 값 으로 원 입력 값 을 유추 하 ᆯ 수 없 도록 하 ᆫ다 . --- 27 ) Chaudhuri , Kamalika , and Claire Monteleoni , “ Privacy - preserving logistic regression ” , 2009. 28 ) Papernot , Nicolas , et al. , “ Scalable private learning with pate ” , 2018.