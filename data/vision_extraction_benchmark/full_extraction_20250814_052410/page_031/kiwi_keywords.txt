markdown AI 학습데이터 모델 보안 관리 적대적공격 화이트 박스 블랙박스 기반 공격 분류 화이트 박스 공격 모델 아키텍처 입력 출력 가중 파라미터 모델 대하 정보 사전 파악 상태 적대적예제 생성 블랙박스 공격 작위 적대적예제 생성 방식 FGSM Deepfool JSMA CW BPDA 연구 적대적예제 생성 학습 AI 모델 확보 방안 적대적공격 사전 예방 일종 데이터증강 Data Augmentation 방식 다량 데이터 학습 AI 모델 높이 이미지 데이터 경우 회전 패딩 필터 방식 데이터 변환 적대적예제 생성 학습 데이터 확장 학습 재현 데이터 기법 활용 확장 데이터 학습 원본 데이터 통계 특성 활용 AI 기법 이용 생성 모의 데이터 확장 데이터 학습 방식 통하 AI 모델 개선 Ian J. Goodfellow et. al. Explaining and Harnessing Adversarial Examples Moosavi Dezfooli Seyed Mohsen Alhussein Fawzi and Pascal Frossard DeepFool a simple and accurate method to fool deep neural networks Papernot Nicolas et al. The Limitations of Deep Learning in Adversarial Settings Carlini Nicolas and David Wagner Towards Evaluating the Robustness of Neural Networks Anish Athalye at al. Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples 문서 페이지 AI 모델 적대적공격 예방 위하 다양 방법 대하 설명 데이터 확장 학습 대하 내용 포함 마지막 가지 관련 연구 논문 인용