``` markdown 제 3 장 . AI 학습데이터 및 모델 보안 관리 - 적대적공격 은 화이트 박스 와 블랙박스 기반 의 공격 으로 분류 하 ᆫ다 . - 화이트 박스 공격 은 모델 의 아키텍처 , 입력 · 출력 값 , 가중 치 , 파라미터 등 모델 에 대하 ᆫ 정보 를 사전 에 파악 하 ᆫ 상태 에서 적대적예제 를 생성 하 ᆫ다 . - 블랙박스 공격 은 무 작위 적 으로 적대적예제 를 생성 하 는 방식 으로 FGSM 2 ) , Deepfool 3 ) , JSMA 4 ) , CW 5 ) , BPDA 6 ) 등 의 연구 가 있 다 . - 적대적예제 를 생성 하 어 학습 하 는 것 은 AI 모델 의 강건 성 을 확보 하 ᆯ 수 있 는 방안 으로 쓰 어 적대적공격 을 사전 에 예방 하 ᆫ다 . * 일종 의 데이터증강 ( Data Augmentation ) 방식 으로 다량 데이터 학습 으로 AI 모델 의 강건 성 을 높이 ᆯ 수 있 음 - 이미지 데이터 의 경우 회전 · 패딩 · 필터 등 의 방식 으로 데이터 를 변환 하 어서 적대적예제 를 생성 하 고 학습 하 ᆯ 수 있 다 . ( 4 ) ( 데이터 확장 · 학습 ) 재현 데이터 등 의 기법 을 활용 하 어 확장 하 ᆫ 데이터 를 학습 하 ᆯ 수 있 다 . * 원본 데이터 의 통계 적 특성 을 활용 하 거나 AI 기법 등 을 이용 하 어 생성 하 ᆫ 모의 데이터 - 확장 하 ᆫ 데이터 를 학습 하 는 방식 을 통하 어 AI 모델 의 강건 성 을 개선 하 ᆯ 수 있 다 . 2) Ian J. Goodfellow et. al. , “ Explaining and Harnessing Adversarial Examples ” , 2015. 3) Moosavi - Dezfooli , Seyed - Mohsen , Alhussein Fawzi and Pascal Frossard , “ DeepFool : a simple and accurate method to fool deep neural networks ” , 2016. 4) Papernot , Nicolas , et al. , “ The Limitations of Deep Learning in Adversarial Settings ” , 2016. 5) Carlini , Nicolas , and David Wagner , “ Towards Evaluating the Robustness of Neural Networks ” , 2017. 6 ) Anish Athalye at al. , “ Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples ” , 2018. ``` --- 이 문서 페이지 에서 는 주로 AI 모델 의 적대적공격 과 이 를 예방 하 기 위하 ᆫ 다양 하 ᆫ 방법 에 대하 어 설명 하 고 있 습니다 . 또한 , 데이터 확장 과 학습 에 대하 ᆫ 내용 도 포함 되 어 있 습니다 . 마지막 에 는 몇 가지 관련 연구 논문 들 이 인용 되 었 습니다 .