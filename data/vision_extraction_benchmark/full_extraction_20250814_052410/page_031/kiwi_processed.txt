```markdown
제3 장. AI 학습데이터 및 모델 보안 관리

- 적대적공격은 화이트 박스와 블랙박스 기반의 공격으로 분류한다.
  - 화이트 박스 공격은 모델의 아키텍처, 입력· 출력 값, 가중치, 파라미터 등 모델에 대한 정보를 사전에 파악한 상태에서 적대적예제를 생성한다.
  - 블랙박스 공격은 무작위적으로 적대적예제를 생성하는 방식으로 FGSM2), Deepfool3), JSMA4), CW5), BPDA6) 등의 연구가 있다.
  
- 적대적예제를 생성하여 학습하는 것은 AI 모델의 강건성을 확보할 수 있는 방안으로 써 적대적공격을 사전에 예방한다.
  * 일종의 데이터증강(Data Augmentation) 방식으로 다량 데이터 학습으로 AI 모델의 강건성을 높일 수 있음
  
- 이미지 데이터의 경우 회전· 패딩· 필터 등의 방식으로 데이터를 변환해서 적대적예제를 생성하고 학습할 수 있다.

(4) (데이터 확장· 학습) 재현 데이터 등의 기법을 활용하여 확장한 데이터를 학습할 수 있다.
  * 원본 데이터의 통계적 특성을 활용하거나 AI 기법 등을 이용하여 생성한 모의 데이터
  
- 확장한 데이터를 학습하는 방식을 통해 AI 모델의 강건성을 개선할 수 있다.

2) Ian J. Goodfellow et. al., “Explaining and Harnessing Adversarial Examples”, 2015.
3) Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi and Pascal Frossard, “DeepFool: a simple and accurate method to fool deep neural networks”, 2016.
4) Papernot, Nicolas, et al., “The Limitations of Deep Learning in Adversarial Settings”, 2016.
5) Carlini, Nicolas, and David Wagner, “Towards Evaluating the Robustness of Neural Networks”, 2017.
6) Anish Athalye at al., “Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples”, 2018.
```

---

이 문서 페이지에서는 주로 AI 모델의 적대적공격과 이를 예방하기 위한 다양한 방법에 대해 설명하고 있습니다. 또한, 데이터 확장과 학습에 대한 내용도 포함되어 있습니다. 마지막에는 몇 가지 관련 연구 논문들이 인용되었습니다.