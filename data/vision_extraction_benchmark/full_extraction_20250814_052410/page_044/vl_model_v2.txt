### 모델 오염 공격

#### 정의
모델 오염 공격은 연합학습 시 클라이언트에서 학습·생성된 악의적인 모델이 중앙서버의 AI 모델에 적용되어 성능저하 및 오작동을 유발하는 공격이다.
- 서버가 각 클라이언트에서 전달되는 모델의 신뢰성 및 오염 여부를 확인하는 것이 어렵기 때문에 모델 오염 공격은 탐지하기 어렵다.

#### 방어 방법
FoolsGold 기법, 집계 알고리즘 적용 등을 통해 모델 오염 공격을 예방할 수 있다.
- (집계 알고리즘 적용) 알려진 집계 알고리즘(Krum^20), Bulyan^21), Trimmed Mean^22))을 사용하여 비잔티움 에러를 완화한다.
- (FoolsGold 기법 적용) 연합학습 시 FoolsGold 기법을 통해서 악성 클라이언트를 판별하고 배제한다.^23)
  - 정상 클라이언트와 악성 클라이언트 간 경사도를 비교해서 서로 다른 업데이트 양상을 확인하여 악성을 판별하는 기법
- (Feature Squeezing) AI 모델의 예측 결과와 Squeezer 알고리즘^24)을 적용한 예측 결과를 비교함으로써, 적대적 예제를 탐지한다.^24)
  - 기존 입력값의 인코딩 단순화, 평활화 필터 등을 적용하여 특징 축소

---

**참고 문헌**
20) Peva Balachard, Rachid Guerraoui, Julien Stainer, et al. "Machine Learning with Adversaries : Byzantine Tolerant Gradient Descent", 2017.
21) El Mahdi El Mhamdi, Rachid Guerraoui, and Sebastien Rouault, "The Hidden Vulnerability of Distributed Learning in Byzantium", 2018.
22) Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett, "Byzantine-Robust Distributed Learning : Towards Optimal Statistical Rates", 2018.
23) Fung, Clement, Chris JM Yoon, and Ivan Beschastnikh., "Mitigating Sybils in Federated Learning Poisoning", 2018.
24) Weiling Xu, "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks", 2017.