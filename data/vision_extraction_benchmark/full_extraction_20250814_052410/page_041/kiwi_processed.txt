### 제3 장. AI 학습데이터 및 모델 보안 관리

#### ⑥ (적대적공격 탐지)
적대적공격을 탐지할 수 있는 기법을 적용한다.
- AI 모델은 적대적공격에 취약할 수 있으며 공격이 발생하는 경우 차단 또는 재학습 등의 조치가 필요하므로 적대적공격 탐지 방안을 고려해야한다.
- 적대적공격을 탐지하기 위한 기법으로는 노이즈 탐지[12], 입력 데이터 인코딩[13], 중요도 맵 활용[14] 등이 있다.

#### AI 모델 보안 고려 사항
| AI 개발주기 | 점검 항목 |
| --- | --- |
| 1. 데이터수집 | AI 학습데이터에 대한 정보를 관리하고 있는가? |
| 2. 데이터전처리 | 학습데이터 이상치(Outlier)를 식별· 관리하였는가? |
|  | 학습데이터의 변조 여부를 확인하였는가? |
|  | 예상할 수 있는 적대적예제를 생성· 학습하였는가? |
| 3. 설계· 학습 | AI 모델 설계 시 알고리즘 선택 등에 있어 강건성을 고려하였는가? |
|  | 모델 튜닝 시 강건성을 확보 조치를 하였는가? |
| (사전 학습 활용 시) | 사전 학습 모델을 신뢰할 수 있는 출처로부터 받았는가? |
| (연합학습 활용 시) | 비잔티움 에러를 대처하는 방법을 적용하였는가? |
| 4. 검증· 테스트 | 검증· 테스트 시에 이용한 데이터 셋, AI 모델 매개변수 등의 정보를 관리하고 있는가? |
|  | AI 모델을 대상으로 적대적공격 등을 수행하여 성능 수준을 확인하였는가? |
|  | 최종 출력 값을 확인하였는가? |
|  | 모델의 출력 횟수를 제한하였는가? |
|  | 모델 정보 노출을 최소화하였는가? |
|  | 적대적공격을 탐지할 수 있는 기법을 적용하였는가? |

---

[12] Kwon, Hyun, Hyunsoo Yoon, and Ki-Woong Park., “Acoustic-decoy: Detection of adversarial examples through audio modification on speech recognition system”, 2020.
[13] Weiling Xu, “Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks”, 2017.
[14] G.Ko and G.Lim, “Unsupervised Detection of Adversarial Examples with Model Explanations”, 2011.