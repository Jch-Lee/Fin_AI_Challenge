### 제 3 장 . AI 학습데이터 및 모델 보안 관리 #### ⑥ ( 적대적공격 탐지 ) 적대적공격 을 탐지 하 ᆯ 수 있 는 기법 을 적용 하 ᆫ다 . - AI 모델 은 적대적공격 에 취약 하 ᆯ 수 있 으며 공격 이 발생 하 는 경우 차단 또는 재 학습 등 의 조치 가 필요 하 므로 적대적공격 탐지 방안 을 고려 하 어야 하 ᆫ다 . - 적대적공격 을 탐지 하 기 위하 ᆫ 기법 으로 는 노이즈 탐지 [ 12 ] , 입력 데이터 인코딩 [ 13 ] , 중요도 맵 활용 [ 14 ] 등 이 있 다 . #### AI 모델 보안 고려 사항 | AI 개발주기 | 점검 항목 | | --- | --- | | 1. 데이터수집 | AI 학습데이터 에 대하 ᆫ 정보 를 관리 하 고 있 는가 ? | | 2. 데이터전처리 | 학습데이터 이상 치 ( Outlier ) 를 식별 · 관리 하 었 는가 ? | | | 학습데이터 의 변조 여부 를 확인 하 었 는가 ? | | | 예상 하 ᆯ 수 있 는 적대적예제 를 생성 · 학습 하 었 는가 ? | | 3. 설계 · 학습 | AI 모델 설계 시 알고리즘 선택 등 에 있 어 강건 성 을 고려 하 었 는가 ? | | | 모델 튜닝 시 강건 성 을 확보 조치 를 하 었 는가 ? | | ( 사전 학습 활용 시 ) | 사전 학습 모델 을 신뢰 하 ᆯ 수 있 는 출처 로부터 받 었 는가 ? | | ( 연합학습 활용 시 ) | 비잔티움 에러 를 대처 하 는 방법 을 적용 하 었 는가 ? | | 4. 검증 · 테스트 | 검증 · 테스트 시 에 이용 하 ᆫ 데이터 셋 , AI 모델 매개변수 등 의 정보 를 관리 하 고 있 는가 ? | | | AI 모델 을 대상 으로 적대적공격 등 을 수행 하 어 성능 수준 을 확인 하 었 는가 ? | | | 최종 출력 값 을 확인 하 었 는가 ? | | | 모델 의 출력 횟수 를 제한 하 었 는가 ? | | | 모델 정보 노출 을 최소 화 하 었 는가 ? | | | 적대적공격 을 탐지 하 ᆯ 수 있 는 기법 을 적용 하 었 는가 ? | --- [ 12 ] Kwon , Hyun , Hyunsoo Yoon , and Ki - Woong Park. , “ Acoustic - decoy : Detection of adversarial examples through audio modification on speech recognition system ” , 2020. [ 13 ] Weiling Xu , “ Feature Squeezing : Detecting Adversarial Examples in Deep Neural Networks ” , 2017. [ 14 ] G.Ko and G.Lim , “ Unsupervised Detection of Adversarial Examples with Model Explanations ” , 2011.