### 참고 5. 회피공격 #### 참고 - 회피공격 은 노이즈 를 추가 하 ᆫ 적대 적 데이터 를 입력 하 어 , AI 모델 이 잘못 되 ᆫ 판단 을 하 도록 유도 하 는 공격 이 다 . #### 참고 - 적대적예제 로 인하 ᆫ 오 분류 ! [ Panda ] + . 007 × [ 노이즈 ] = [ Gibbon ] - 회피공격 은 입력 값 에 사람 의 눈 으로 는 구분 하 기 어렵 은 노이즈 의 추가 로 결과 값 에 영향 을 미치 기 때문 에 , 데이터 관찰 을 통하 ᆫ 탐지 가 쉽 지 않 음 #### 적대적예제 학습 , 경사도 마스 킹 등 의 방법 을 통하 어 회피공격 을 예방 하 고 차단 하 ᆯ 수 있 다 . - ( 적대적예제 학습 ) 사전 에 적대적예제 를 생성 하 어 학습 하 ᆷ 으로써 , 적대적예제 를 통하 ᆫ 회피공격 을 방어 하 ᆯ 수 있 다 . - ( 경사도 ) 마스 킹 공격자 는 데이터 를 입력 하 었 을 시 에 AI 모델 의 학습 방향 등 변화 되 는 정도 를 기반 으로 적대적예제 를 생성 하 ᆯ 수 있 다 . - AI 모델 의 학습 방향 은 데이터 입력 시 에 AI 모델 의 경사도 를 통하 어 확인 하 ᆯ 수 있 으며 , 이 를 공개 하 지 않 거나 마스 킹 , 범주 화 등 의 기법 을 활용 하 어 모델 의 정보 노출 을 최소 화 하 ᆫ다 . --- ### 출처 1. Ian J. Goodfellow et. al. , " Explaining and Harnessing Adversarial Examples " , 2015. 2. Zhang , Lemoine , and Mitchell , " Mitigating Unwanted Biases with Adversarial Learning " , 2018. 3. Anish Athalye at al. , " Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples " , 2018.