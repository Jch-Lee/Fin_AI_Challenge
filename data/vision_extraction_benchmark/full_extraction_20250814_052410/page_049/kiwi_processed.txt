### 참고 5. 회피공격

#### 참고
- 회피공격은 노이즈를 추가한 적대적 데이터를 입력하여, AI 모델이 잘못된 판단을 하도록 유도하는 공격이다.

#### 참고
- 적대적예제로 인한 오 분류![Panda] + .007 × [노이즈] = [Gibbon]

- 회피공격은 입력 값에 사람의 눈으로는 구분하기 어려운 노이즈의 추가로 결과 값에 영향을 미치기 때문에, 데이터 관찰을 통한 탐지가 쉽지 않음

#### 적대적예제 학습, 경사도 마스 킹 등의 방법을 통해 회피공격을 예방하고 차단할 수 있다.
- (적대적예제 학습) 사전에 적대적예제를 생성하여 학습함으로써, 적대적예제를 통한 회피공격을 방어할 수 있다.
- (경사도) 마스 킹 공격자는 데이터를 입력하였을 시에 AI 모델의 학습 방향 등 변화되는 정도를 기반으로 적대적예제를 생성할 수 있다.
    - AI 모델의 학습 방향은 데이터 입력 시에 AI 모델의 경사도를 통하여 확인할 수 있으며, 이를 공개하지 않거나 마스 킹, 범주화 등의 기법을 활용하여 모델의 정보 노출을 최소화한다.

---

### 출처
1. Ian J. Goodfellow et. al., "Explaining and Harnessing Adversarial Examples", 2015.
2. Zhang, Lemoine, and Mitchell, "Mitigating Unwanted Biases with Adversarial Learning", 2018.
3. Anish Athalye at al., "Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples", 2018.