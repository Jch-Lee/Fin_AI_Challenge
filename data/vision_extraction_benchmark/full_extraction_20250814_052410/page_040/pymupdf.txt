●●●금융분야 AI 보안 가이드라인
36
□
3  (최종 출력값 확인) 개인정보 등 민감한 정보가 이용자에게 출력되는지 확인한다.
민감한 정보가 출력되는 경우, 불특정 다수 또는 타인에게 노출이 되지 않도록 하는 
방안을 마련한다.
※ AI 학습 데이터의 개인정보 활용과 관련된 상세한 사항은 개인정보보호법 및 인공지능
(AI) 개인정보보호 자율점검표(’21.5., 개인정보위) 참조
□
4  (출력 횟수 제한) AI 모델의 출력 횟수를 제한하여 모델의 정보 및 학습 데이터 유추를 
어렵게 한다.
공격자는 AI 모델의 입･출력값을 기반으로 모델의 정보를 유추하고, 공격을 시도한다.
※ [참고3] 모델 추출 공격, [참고4] 모델 인버전 공격, [참고5] 회피 공격 참조
실제로 입･출력값 수집을 통해 아마존과 BigML의 유료 AI 모델을 99% 이상의 
유사성으로 복제한 사례가 있다.11)
AI 모델 정보의 유추와 모델 복제를 막기 위해서는 입･출력 횟수와 시간을 제한하는 
방법 등을 적용할 수 있다.
출력 횟수를 제한하는 경우 AI 모델이 사용되는 업무나 서비스 특성을 고려*해 가급적 
낮은 수준으로 정한다.
* (예시) 챗봇의 경우 1분에 10회 미만 등
□
5  (신뢰 점수 제한) 정확도, 신뢰 점수* 등을 공개하지 않거나 범주화 등 비식별하여 제공
함으로써 학습 데이터 및 모델 관련 정보 노출을 최소화한다.
* AI 모델이 데이터를 얼마나 믿을 수 있는지에 대한 점수 척도
공격자가 출력 결과에 대한 정확성, 신뢰 점수 등 모델에 대한 정보를 획득하면 모델 
복제를 더 쉽게 할 수 있다.
※ [참고3] 모델 추출 공격, [참고4] 모델 인버전 공격, [참고5] 회피 공격 참조
따라서, 모델에 대한 정보 노출을 최소한으로 제한하여 공격을 어렵게 할 필요가 있다.
※ 다만, AI 의사결정 과정에 대한 설명의무가 있는 경우 설명에 불필요한 정보의 제공을 
최소화한다.
11) Tramer et.al., “Stealing Machine Learning Models via Prediction APIs”, 2016.
