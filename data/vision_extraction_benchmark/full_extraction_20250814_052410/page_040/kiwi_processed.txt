```markdown
# 금융분야 AI 보안 가이드라인

## ③ 최종 출력 값 확인 개인 정보 등 민감한 정보가 이용자에게 출력되는지 확인한다.
- 민감한 정보가 출력되는 경우, 불특정 다수 또는 타인에게 노출이 되지 않도록하는 방안을 마련한다.
※ AI 학습데이터의 개인 정보 활용과 관련된 상세한 사항은 개인정보보호 법 및 인공지능(AI) 개인정보보호 자율 점검 표('21.5., 개인 정보위) 참조

## ④ 출력 횟수 제한
AI 모델의 출력 횟수를 제한하여 모델의 정보 및 학습데이터 유추를 어렵게한다.
- 공격자는 AI 모델의 입· 출력 값을 기반으로 모델의 정보를 유추하고, 공격을 시도한다.
※ [참고 3] 모델추출 공격, [참고 4] 모델인 버전 공격, [참고 5] 회피공격 참조
- 실제로 입· 출력 값 수집을 통해 아마존과 BigML의 유료 AI 모델을 99 % 이상의 유사성으로 복제한 사례가 있다.[11]
- AI 모델 정보의 유추와 모델 복제를 막기 위해서는 입· 출력 횟수와 시간을 제한하는 방법 등을 적용할 수 있다.
- 출력 횟수를 제한하는 경우 AI 모델이 사용되는 업무나 서비스 특성을 고려해 가급적 낮은 수준으로 정한다.
*(예시) 챗봇의 경우 1분에 10회 미만 등

## ⑤ 신뢰 점수 제한 정확도, 신뢰 점수 * 등을 공개하지 않거나 범주화 등 비식별하여 제공함으로써 학습데이터 및 모델 관련 정보 노출을 최소화한다.
* AI 모델이 데이터를 얼마나 믿을 수 있는지에 대한 점수 척도
- 공격자가 출력 결과에 대한 정확성, 신뢰 점수 등 모델에 대한 정보를 획득하면 모델 복제를 더 쉽게 할 수 있다.
※ [참고 3] 모델추출 공격, [참고 4] 모델인 버전 공격, [참고 5] 회피공격 참조
- 따라서, 모델에 대한 정보 노출을 최소한으로 제한하여 공격을 어렵게할 필요가 있다.
※ 다만, AI 의사 결정 과정에 대한 설명 의무가 있는 경우 설명에 불필요한 정보의 제공을 최소화한다.

[11] Tramer et.al., “Stealing Machine Learning Models via Prediction APIs”, 2016.
```

---

# 참고 자료

- [참고 3] 모델추출 공격
- [참고 4] 모델인 버전 공격
- [참고 5] 회피공격
- "Stealing Machine Learning Models via Prediction APIs", Tramer et.al., 2016