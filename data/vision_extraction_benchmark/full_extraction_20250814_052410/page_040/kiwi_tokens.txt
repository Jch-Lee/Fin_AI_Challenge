``` markdown # 금융분야 AI 보안 가이드라인 ## ③ 최종 출력 값 확인 개인 정보 등 민감 하 ᆫ 정보 가 이용자 에게 출력 되 는지 확인 하 ᆫ다 . - 민감 하 ᆫ 정보 가 출력 되 는 경우 , 불 특정 다수 또는 타인 에게 노출 이 되 지 않 도록 하 는 방안 을 마련 하 ᆫ다 . ※ AI 학습데이터 의 개인 정보 활용 과 관련 되 ᆫ 상세 하 ᆫ 사항 은 개인정보보호 법 및 인공지능 ( AI ) 개인정보보호 자율 점검 표 ( ' 21.5. , 개인 정보위 ) 참조 ## ④ 출력 횟수 제한 AI 모델 의 출력 횟수 를 제한 하 어 모델 의 정보 및 학습데이터 유추 를 어렵 게 하 ᆫ다 . - 공격자 는 AI 모델 의 입 · 출력 값 을 기반 으로 모델 의 정보 를 유추 하 고 , 공격 을 시도 하 ᆫ다 . ※ [ 참고 3 ] 모델추출 공격 , [ 참고 4 ] 모델 이 ᆫ 버전 공격 , [ 참고 5 ] 회피공격 참조 - 실제로 입 · 출력 값 수집 을 통하 어 아마존 과 BigML 의 유료 AI 모델 을 99 % 이상 의 유사 성 으로 복제 하 ᆫ 사례 가 있 다 . [ 11 ] - AI 모델 정보 의 유추 와 모델 복제 를 막 기 위하 어서 는 입 · 출력 횟수 와 시간 을 제한 하 는 방법 등 을 적용 하 ᆯ 수 있 다 . - 출력 횟수 를 제한 하 는 경우 AI 모델 이 사용 되 는 업무 나 서비스 특성 을 고려 하 어 가급적 낮 은 수준 으로 정하 ᆫ다 . * ( 예시 ) 챗봇 의 경우 1 분 에 10 회 미만 등 ## ⑤ 신뢰 점수 제한 정확도 , 신뢰 점수 * 등 을 공개 하 지 않 거나 범주 화 등 비 식별 하 어 제공 하 ᆷ 으로써 학습데이터 및 모델 관련 정보 노출 을 최소 화 하 ᆫ다 . * AI 모델 이 데이터 를 얼마나 믿 을 수 있 는지 에 대하 ᆫ 점수 척도 - 공격자 가 출력 결과 에 대하 ᆫ 정확 성 , 신뢰 점수 등 모델 에 대하 ᆫ 정보 를 획득 하 면 모델 복제 를 더 쉽 게 하 ᆯ 수 있 다 . ※ [ 참고 3 ] 모델추출 공격 , [ 참고 4 ] 모델 이 ᆫ 버전 공격 , [ 참고 5 ] 회피공격 참조 - 따라서 , 모델 에 대하 ᆫ 정보 노출 을 최소한 으로 제한 하 어 공격 을 어렵 게 하 ᆯ 필요 가 있 다 . ※ 다만 , AI 의사 결정 과정 에 대하 ᆫ 설명 의무 가 있 는 경우 설명 에 불 필요 하 ᆫ 정보 의 제공 을 최소 화 하 ᆫ다 . [ 11 ] Tramer et.al. , “ Stealing Machine Learning Models via Prediction APIs ” , 2016. ``` --- # 참고 자료 - [ 참고 3 ] 모델추출 공격 - [ 참고 4 ] 모델 이 ᆫ 버전 공격 - [ 참고 5 ] 회피공격 - " Stealing Machine Learning Models via Prediction APIs " , Tramer et.al. , 2016