참고 3. 모델 추출 공격●●●
41
참고3
▶모델 추출 공격
모델 추출 공격은 원본 AI 모델로부터 유사한 모델을 추출하여 복제하는 공격이다.
원본 AI 모델에 대한 대량의 쿼리를 통해 입･출력값을 수집하고 이를 학습하여 원본 
AI 모델과 유사도가 높은 모델을 복제한다.
참고
모델 추출 공격 프로세스25)
아마존(Amazon) 및 BigML이 MLaaS 형태로 제공하는 유료 AI 모델을 몇 분만에 
99% 이상 유사도로 복제 가능하다는 것이 관련 연구25)로 증명된 바 있다.
다음과 같은 방법을 통해서 모델 추출 공격을 예방할 수 있다.
(출력값 제한) 출력 횟수와 시간을 제한하여 공격자의 입･출력 정보 수집을 어렵게 
한다.
(예측 결과 변환) 비식별 처리 등을 통해 출력값을 변환하면, 입･출력값으로부터 
유사 모델을 생성하기 어려워져 모델 추출 공격을 예방할 수 있다.
(DP-SGD 기법*26)) 학습 과정에서 차등 프라이버시 기법을 적용함으로써, 경사도 
등 모델과 관련된 정보 노출을 최소화하는 기법이다.
25) Tramer et.al., “Stealing Machine Learning Models via Prediction APIs”, 2016.
26) Martin Abadi et al., “Deep Learning with Differential Privacy”, 2016.
