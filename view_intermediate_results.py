"""
RAG ì‹œìŠ¤í…œ ì¤‘ê°„ ìƒì„±ë¬¼ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import io
import json
import numpy as np
import pickle
from pathlib import Path

# UTF-8 ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


def view_logs():
    """ë¡œê·¸ íŒŒì¼ í™•ì¸"""
    print("\n=== ë¡œê·¸ íŒŒì¼ ===")
    log_dir = Path("logs")
    if log_dir.exists():
        for log_file in log_dir.glob("*.log"):
            print(f"\nğŸ“„ {log_file.name}")
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"   ì´ {len(lines)}ì¤„")
                # ë§ˆì§€ë§‰ 5ì¤„ ì¶œë ¥
                print("   ë§ˆì§€ë§‰ 5ì¤„:")
                for line in lines[-5:]:
                    print(f"   {line.strip()}")


def view_test_results():
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ JSON í™•ì¸"""
    print("\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    log_dir = Path("logs")
    if log_dir.exists():
        for json_file in log_dir.glob("*.json"):
            print(f"\nğŸ“Š {json_file.name}")
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ì‹œìŠ¤í…œ ì •ë³´
                if 'system_info' in data:
                    print("   ì‹œìŠ¤í…œ ì •ë³´:")
                    print(f"   - Device: {data['system_info'].get('device', 'N/A')}")
                    print(f"   - GPU: {data['system_info'].get('gpu_name', 'N/A')}")
                
                # íŒŒì´í”„ë¼ì¸ ì„¤ì •
                if 'pipeline_config' in data:
                    print("   íŒŒì´í”„ë¼ì¸ ì„¤ì •:")
                    config = data['pipeline_config']
                    print(f"   - Embedder: {config.get('embedder_model', 'N/A')}")
                    print(f"   - Retriever: {config.get('retriever_type', 'N/A')}")
                    print(f"   - Reranking: {config.get('reranking_enabled', False)}")
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                if 'performance_metrics' in data:
                    print("   ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
                    metrics = data['performance_metrics']
                    print(f"   - ì„ë² ë”© í‰ê· : {metrics.get('embedding_avg', 0):.3f}ì´ˆ")
                    print(f"   - ê²€ìƒ‰ í‰ê· : {metrics.get('retrieval_avg', 0):.3f}ì´ˆ")
                    print(f"   - ì „ì²´ í‰ê· : {metrics.get('total_avg', 0):.3f}ì´ˆ")


def view_knowledge_base():
    """ì§€ì‹ ë² ì´ìŠ¤ ë‚´ìš© í™•ì¸"""
    print("\n=== ì§€ì‹ ë² ì´ìŠ¤ ===")
    
    kb_files = [
        "pdf_knowledge_base.pkl",
        "test_knowledge_base.pkl"
    ]
    
    for kb_file in kb_files:
        kb_path = Path(kb_file)
        if kb_path.exists():
            if kb_path.is_dir():
                pkl_file = kb_path / "knowledge_base.pkl"
                if pkl_file.exists():
                    kb_path = pkl_file
                else:
                    continue
            
            print(f"\nğŸ’¾ {kb_path}")
            try:
                with open(kb_path, 'rb') as f:
                    kb = pickle.load(f)
                    
                    if hasattr(kb, 'documents'):
                        print(f"   ë¬¸ì„œ ìˆ˜: {len(kb.documents)}")
                        if kb.documents:
                            print("   ì²« ë²ˆì§¸ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:")
                            doc = kb.documents[0]
                            if isinstance(doc, dict):
                                content = doc.get('content', '')[:100]
                            else:
                                content = str(doc)[:100]
                            print(f"   {content}...")
                    
                    if hasattr(kb, 'index'):
                        print(f"   ì¸ë±ìŠ¤ í¬ê¸°: {kb.index.ntotal if hasattr(kb.index, 'ntotal') else 'N/A'}")
            except Exception as e:
                print(f"   âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")


def view_embeddings():
    """ì„ë² ë”© ë°ì´í„° í™•ì¸"""
    print("\n=== ì„ë² ë”© ë°ì´í„° ===")
    
    embedding_dirs = [
        Path("data/kure_embeddings/latest"),
        Path("data/e5_embeddings/latest")
    ]
    
    for emb_dir in embedding_dirs:
        if emb_dir.exists():
            print(f"\nğŸ“ {emb_dir}")
            
            # chunks.json
            chunks_file = emb_dir / "chunks.json"
            if chunks_file.exists():
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks = json.load(f)
                    print(f"   ì²­í¬ ìˆ˜: {len(chunks)}")
                    if chunks:
                        # chunksê°€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                        first_chunk = chunks[0]
                        if isinstance(first_chunk, dict):
                            content = first_chunk.get('content', str(first_chunk))[:100]
                        else:
                            content = str(first_chunk)[:100]
                        print(f"   ì²« ì²­í¬: {content}...")
            
            # embeddings.npy
            emb_file = emb_dir / "embeddings.npy"
            if emb_file.exists():
                embeddings = np.load(emb_file)
                print(f"   ì„ë² ë”© shape: {embeddings.shape}")
                print(f"   ì„ë² ë”© ì°¨ì›: {embeddings.shape[1] if len(embeddings.shape) > 1 else 'N/A'}")
            
            # metadata.json
            meta_file = emb_dir / "metadata.json"
            if meta_file.exists():
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    print(f"   ë©”íƒ€ë°ì´í„° í‚¤: {list(metadata.keys())}")


def view_pipeline_results():
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ í™•ì¸"""
    print("\n=== íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ===")
    
    pipeline_dir = Path("pipeline_results")
    if pipeline_dir.exists():
        files = sorted(pipeline_dir.glob("*"))
        for file in files:
            print(f"\nğŸ“„ {file.name}")
            
            if file.suffix == '.txt':
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    print(f"   ê¸¸ì´: {len(content)} ë¬¸ì")
                    print(f"   ë¯¸ë¦¬ë³´ê¸°: {content[:100]}...")
            
            elif file.suffix == '.json':
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        print(f"   í•­ëª© ìˆ˜: {len(data)}")
                        if data:
                            print(f"   ì²« í•­ëª©: {str(data[0])[:100]}...")
                    elif isinstance(data, dict):
                        print(f"   í‚¤: {list(data.keys())}")
            
            elif file.suffix == '.npy':
                arr = np.load(file)
                print(f"   Shape: {arr.shape}")
                print(f"   dtype: {arr.dtype}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("RAG ì‹œìŠ¤í…œ ì¤‘ê°„ ìƒì„±ë¬¼ í™•ì¸")
    print("="*60)
    
    # 1. ë¡œê·¸ íŒŒì¼
    view_logs()
    
    # 2. í…ŒìŠ¤íŠ¸ ê²°ê³¼
    view_test_results()
    
    # 3. ì§€ì‹ ë² ì´ìŠ¤
    view_knowledge_base()
    
    # 4. ì„ë² ë”© ë°ì´í„°
    view_embeddings()
    
    # 5. íŒŒì´í”„ë¼ì¸ ê²°ê³¼
    view_pipeline_results()
    
    print("\n" + "="*60)
    print("âœ… ì¤‘ê°„ ìƒì„±ë¬¼ í™•ì¸ ì™„ë£Œ")
    print("="*60)


if __name__ == "__main__":
    main()