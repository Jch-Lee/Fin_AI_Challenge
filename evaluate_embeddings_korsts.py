"""
KorSTS ë²¤ì¹˜ë§ˆí¬ë¥¼ í™œìš©í•œ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
ê¸ˆìœµë³´ì•ˆ ë„ë©”ì¸ RAG ì‹œìŠ¤í…œì˜ ì„ë² ë”© í’ˆì§ˆ ê²€ì¦
"""

import torch
import numpy as np
from scipy.stats import spearmanr, pearsonr
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
import logging
from typing import Dict, List, Tuple
import json
from pathlib import Path
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KorSTSEvaluator:
    """KorSTS ë°ì´í„°ì…‹ì„ ì´ìš©í•œ ì„ë² ë”© ëª¨ë¸ í‰ê°€"""
    
    def __init__(self, model_name: str = "nlpai-lab/KURE-v1", device: str = None):
        """
        Args:
            model_name: í‰ê°€í•  ì„ë² ë”© ëª¨ë¸
            device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        """
        self.model_name = model_name
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"Initializing evaluator for model: {model_name}")
        logger.info(f"Using device: {self.device}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer(model_name, device=self.device)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"Model loaded. Embedding dimension: {self.embedding_dim}")
    
    def load_korsts_dataset(self) -> Tuple[List[str], List[str], List[float]]:
        """KorSTS ë°ì´í„°ì…‹ ë¡œë“œ"""
        logger.info("Loading KorSTS dataset...")
        
        try:
            # KorSTS ë°ì´í„°ì…‹ ë¡œë“œ (KLUE ë²¤ì¹˜ë§ˆí¬ì˜ ì¼ë¶€)
            dataset = load_dataset("klue", "sts", split="validation")
            
            sentences1 = []
            sentences2 = []
            scores = []
            
            for item in dataset:
                sentences1.append(item['sentence1'])
                sentences2.append(item['sentence2'])
                # KorSTS ì ìˆ˜ëŠ” 0-5 ë²”ìœ„, ì •ê·œí™”í•˜ì—¬ 0-1ë¡œ ë³€í™˜
                scores.append(item['labels']['label'] / 5.0)
            
            logger.info(f"Loaded {len(sentences1)} sentence pairs from KorSTS")
            return sentences1, sentences2, scores
            
        except Exception as e:
            logger.warning(f"Failed to load official KorSTS: {e}")
            logger.info("Creating sample test data for evaluation...")
            
            # ëŒ€ì²´ ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš©)
            sentences1 = [
                "ê¸ˆìœµ ë³´ì•ˆì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.",
                "ë¹„ë°€ë²ˆí˜¸ëŠ” ì •ê¸°ì ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.",
                "ì€í–‰ ê³„ì¢Œë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ì„¸ìš”.",
                "íˆ¬ìí•  ë•ŒëŠ” ìœ„í—˜ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.",
                "ë””ì§€í„¸ ìì‚°ì„ ë³´í˜¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
            ]
            sentences2 = [
                "ê¸ˆìœµ ë³´ì•ˆì˜ ì¤‘ìš”ì„±ì€ ë§¤ìš° í½ë‹ˆë‹¤.",
                "íŒ¨ìŠ¤ì›Œë“œë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ë°”ê¿”ì•¼ í•©ë‹ˆë‹¤.",
                "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤.",
                "íˆ¬ì ì‹œ ë¦¬ìŠ¤í¬ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "ì•”í˜¸í™”íë¥¼ ì•ˆì „í•˜ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆë‹¤."
            ]
            scores = [0.9, 0.95, 0.1, 0.85, 0.8]
            
            return sentences1, sentences2, scores
    
    def evaluate_embeddings(self, 
                           sentences1: List[str], 
                           sentences2: List[str], 
                           true_scores: List[float],
                           batch_size: int = 32) -> Dict:
        """ì„ë² ë”© í’ˆì§ˆ í‰ê°€"""
        logger.info(f"Evaluating embeddings for {len(sentences1)} pairs...")
        
        # ì„ë² ë”© ìƒì„±
        embeddings1 = self.model.encode(
            sentences1, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        embeddings2 = self.model.encode(
            sentences2,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        predicted_scores = []
        for emb1, emb2 in zip(embeddings1, embeddings2):
            similarity = np.dot(emb1, emb2)  # ì´ë¯¸ ì •ê·œí™”ë¨
            predicted_scores.append(similarity)
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        spearman_corr, spearman_p = spearmanr(true_scores, predicted_scores)
        pearson_corr, pearson_p = pearsonr(true_scores, predicted_scores)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        mse = np.mean((np.array(true_scores) - np.array(predicted_scores)) ** 2)
        mae = np.mean(np.abs(np.array(true_scores) - np.array(predicted_scores)))
        
        results = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'num_pairs': len(sentences1),
            'spearman_correlation': float(spearman_corr),
            'spearman_p_value': float(spearman_p),
            'pearson_correlation': float(pearson_corr),
            'pearson_p_value': float(pearson_p),
            'mse': float(mse),
            'mae': float(mae),
            'performance_grade': self._get_performance_grade(spearman_corr)
        }
        
        return results
    
    def _get_performance_grade(self, spearman_corr: float) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ íŒì •"""
        if spearman_corr >= 0.8:
            return "Excellent (ë§¤ìš° ìš°ìˆ˜)"
        elif spearman_corr >= 0.7:
            return "Good (ìš°ìˆ˜)"
        elif spearman_corr >= 0.6:
            return "Fair (ì–‘í˜¸)"
        elif spearman_corr >= 0.5:
            return "Poor (ê°œì„  í•„ìš”)"
        else:
            return "Very Poor (ì¬í›ˆë ¨ í•„ìš”)"
    
    def evaluate_financial_domain(self) -> Dict:
        """ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í‰ê°€"""
        logger.info("Evaluating financial domain performance...")
        
        # ê¸ˆìœµ ë„ë©”ì¸ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìŒ
        financial_pairs = [
            ("ê¸ˆë¦¬ ì¸ìƒì´ ì˜ˆìƒë©ë‹ˆë‹¤", "ì´ììœ¨ì´ ì˜¤ë¥¼ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤", 0.9),
            ("ì£¼ì‹ ì‹œì¥ì´ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤", "ì¦ê¶Œ ê±°ë˜ì†Œê°€ í™œí™©ì…ë‹ˆë‹¤", 0.85),
            ("ëŒ€ì¶œ ìƒí™˜ì´ í•„ìš”í•©ë‹ˆë‹¤", "ìœµìê¸ˆì„ ê°šì•„ì•¼ í•©ë‹ˆë‹¤", 0.95),
            ("íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë‹¤ê°í™”í•˜ì„¸ìš”", "ë¶„ì‚° íˆ¬ìê°€ ì¤‘ìš”í•©ë‹ˆë‹¤", 0.8),
            ("ì˜ˆê¸ˆ ê¸ˆë¦¬ê°€ ì¸í•˜ë˜ì—ˆìŠµë‹ˆë‹¤", "ì €ì¶• ì´ìê°€ ë‚®ì•„ì¡ŒìŠµë‹ˆë‹¤", 0.9),
            ("ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì´ ê¸‰ë“±í–ˆìŠµë‹ˆë‹¤", "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§‘ìŠµë‹ˆë‹¤", 0.1),
            ("ì‹ ìš©ì¹´ë“œ ì‚¬ìš©ì„ ì¤„ì´ì„¸ìš”", "ì¹´ë“œ ê²°ì œë¥¼ ìì œí•˜ì„¸ìš”", 0.95),
            ("ë³´ì•ˆ í† í°ì„ ìƒì„±í•˜ì„¸ìš”", "OTPë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤", 0.85)
        ]
        
        sentences1 = [pair[0] for pair in financial_pairs]
        sentences2 = [pair[1] for pair in financial_pairs]
        true_scores = [pair[2] for pair in financial_pairs]
        
        # í‰ê°€ ìˆ˜í–‰
        results = self.evaluate_embeddings(sentences1, sentences2, true_scores)
        results['domain'] = 'financial'
        
        return results
    
    def run_comprehensive_evaluation(self) -> Dict:
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        logger.info("="*60)
        logger.info("Starting comprehensive embedding evaluation")
        logger.info("="*60)
        
        all_results = {}
        
        # 1. KorSTS ë²¤ì¹˜ë§ˆí¬ í‰ê°€
        try:
            sentences1, sentences2, scores = self.load_korsts_dataset()
            korsts_results = self.evaluate_embeddings(sentences1, sentences2, scores)
            all_results['korsts'] = korsts_results
            
            logger.info("\nğŸ“Š KorSTS Benchmark Results:")
            logger.info(f"  - Spearman Correlation: {korsts_results['spearman_correlation']:.4f}")
            logger.info(f"  - Pearson Correlation: {korsts_results['pearson_correlation']:.4f}")
            logger.info(f"  - Performance Grade: {korsts_results['performance_grade']}")
            
        except Exception as e:
            logger.error(f"KorSTS evaluation failed: {e}")
            all_results['korsts'] = {'error': str(e)}
        
        # 2. ê¸ˆìœµ ë„ë©”ì¸ í‰ê°€
        try:
            financial_results = self.evaluate_financial_domain()
            all_results['financial'] = financial_results
            
            logger.info("\nğŸ’° Financial Domain Results:")
            logger.info(f"  - Spearman Correlation: {financial_results['spearman_correlation']:.4f}")
            logger.info(f"  - Performance Grade: {financial_results['performance_grade']}")
            
        except Exception as e:
            logger.error(f"Financial domain evaluation failed: {e}")
            all_results['financial'] = {'error': str(e)}
        
        # 3. ì¢…í•© íŒì •
        self._print_summary(all_results)
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(all_results)
        
        return all_results
    
    def _print_summary(self, results: Dict):
        """í‰ê°€ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ˆ EVALUATION SUMMARY")
        logger.info("="*60)
        
        if 'korsts' in results and 'spearman_correlation' in results['korsts']:
            spearman = results['korsts']['spearman_correlation']
            grade = results['korsts']['performance_grade']
            
            if spearman >= 0.7:
                logger.info(f"âœ… ëª¨ë¸ ì„±ëŠ¥: {grade}")
                logger.info(f"   ìƒê´€ê³„ìˆ˜ {spearman:.4f} â‰¥ 0.7")
                logger.info("   â†’ í•œêµ­ì–´ ë¬¸ì„œ ì„ë² ë”©ì´ ìš°ìˆ˜í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
            else:
                logger.info(f"âš ï¸ ëª¨ë¸ ì„±ëŠ¥: {grade}")
                logger.info(f"   ìƒê´€ê³„ìˆ˜ {spearman:.4f} < 0.7")
                logger.info("   â†’ ì¶”ê°€ í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        logger.info("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        if 'financial' in results and 'spearman_correlation' in results['financial']:
            fin_score = results['financial']['spearman_correlation']
            if fin_score >= 0.8:
                logger.info("  - ê¸ˆìœµ ë„ë©”ì¸ ì„±ëŠ¥ ìš°ìˆ˜, RAG ì‹œìŠ¤í…œì— ë°”ë¡œ ì ìš© ê°€ëŠ¥")
            else:
                logger.info("  - ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” íŒŒì¸íŠœë‹ ê³ ë ¤")
    
    def _save_results(self, results: Dict):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        output_dir = Path("evaluation_results")
        output_dir.mkdir(exist_ok=True)
        
        output_file = output_dir / f"korsts_evaluation_{self.model_name.replace('/', '_')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ“ Results saved to: {output_file}")


def compare_models(model_names: List[str]) -> Dict:
    """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í‰ê°€"""
    logger.info("Comparing multiple models...")
    
    comparison_results = {}
    
    for model_name in model_names:
        logger.info(f"\nEvaluating: {model_name}")
        try:
            evaluator = KorSTSEvaluator(model_name)
            results = evaluator.run_comprehensive_evaluation()
            comparison_results[model_name] = results
        except Exception as e:
            logger.error(f"Failed to evaluate {model_name}: {e}")
            comparison_results[model_name] = {'error': str(e)}
    
    # ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š MODEL COMPARISON")
    logger.info("="*60)
    
    for model_name, results in comparison_results.items():
        if 'korsts' in results and 'spearman_correlation' in results['korsts']:
            score = results['korsts']['spearman_correlation']
            logger.info(f"{model_name}: {score:.4f}")
    
    return comparison_results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # KURE vs E5 ëª¨ë¸ ë¹„êµ í‰ê°€
    models_to_evaluate = [
        "nlpai-lab/KURE-v1",  # í˜„ì¬ ë©”ì¸ ëª¨ë¸ (1024ì°¨ì›)
        "dragonkue/multilingual-e5-small-ko",  # ì´ì „ ëª¨ë¸ (384ì°¨ì›)
    ]
    
    for model_name in models_to_evaluate:
        logger.info(f"\n{'='*60}")
        logger.info(f"Evaluating: {model_name}")
        logger.info(f"{'='*60}")
        
        try:
            evaluator = KorSTSEvaluator(model_name)
            results = evaluator.run_comprehensive_evaluation()
            
            # ì„±ê³µ/ì‹¤íŒ¨ íŒì •
            if 'korsts' in results and 'spearman_correlation' in results['korsts']:
                if results['korsts']['spearman_correlation'] >= 0.7:
                    logger.info(f"{model_name}: ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼!")
                else:
                    logger.info(f"{model_name}: ì„±ëŠ¥ ê°œì„  í•„ìš”")
                    
        except Exception as e:
            logger.error(f"Evaluation failed for {model_name}: {e}")
    
    # ì „ì²´ ëª¨ë¸ ë¹„êµ ì‹¤í–‰
    logger.info("\n" + "="*60)
    logger.info("COMPREHENSIVE MODEL COMPARISON")
    logger.info("="*60)
    comparison_results = compare_models(models_to_evaluate)


if __name__ == "__main__":
    main()