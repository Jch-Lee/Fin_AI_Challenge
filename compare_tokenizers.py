#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Kiwi vs KURE í† í¬ë‚˜ì´ì € ë¹„êµ ì‹¤í—˜
- Kiwi: í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì§•
- KURE: SentenceTransformer ë‚´ì¥ í† í¬ë‚˜ì´ì €
"""

import sys
import io
import time
from typing import List, Dict, Any
import re

# UTF-8 ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Kiwi ì„í¬íŠ¸
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    print("Kiwi not available")

# KURE ì„í¬íŠ¸
try:
    from sentence_transformers import SentenceTransformer
    import torch
    KURE_AVAILABLE = True
except ImportError:
    KURE_AVAILABLE = False
    print("SentenceTransformer not available")

def test_kiwi_tokenizer(texts: List[str]) -> Dict[str, Any]:
    """Kiwi í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    if not KIWI_AVAILABLE:
        return {"error": "Kiwi not available"}
    
    kiwi = Kiwi()
    results = {
        "name": "Kiwi",
        "method": "í˜•íƒœì†Œ ë¶„ì„",
        "tokens": [],
        "stats": [],
        "processing_time": 0
    }
    
    start_time = time.time()
    
    for text in texts:
        # ë„ì–´ì“°ê¸° êµì •
        corrected_text = kiwi.space(text)
        
        # í˜•íƒœì†Œ ë¶„ì„
        morphemes = kiwi.tokenize(text)
        
        # ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œ (BM25ìš©)
        meaningful_tokens = []
        for token in morphemes:
            if token.tag.startswith(('N', 'V', 'VA', 'SL')):  # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì™¸êµ­ì–´
                if len(token.form) >= 2 or token.tag.startswith('SL'):
                    meaningful_tokens.append(token.form)
        
        # ëª¨ë“  í˜•íƒœì†Œ (ë¶„ì„ìš©)
        all_tokens = [f"{token.form}/{token.tag}" for token in morphemes]
        
        results["tokens"].append({
            "original": text,
            "corrected": corrected_text,
            "meaningful_tokens": meaningful_tokens,
            "all_morphemes": all_tokens,
            "token_count": len(meaningful_tokens),
            "morpheme_count": len(all_tokens)
        })
        
        results["stats"].append({
            "char_count": len(text),
            "meaningful_tokens": len(meaningful_tokens),
            "total_morphemes": len(all_tokens),
            "avg_token_length": sum(len(t) for t in meaningful_tokens) / max(len(meaningful_tokens), 1)
        })
    
    results["processing_time"] = time.time() - start_time
    return results

def test_kure_tokenizer(texts: List[str]) -> Dict[str, Any]:
    """KURE í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    if not KURE_AVAILABLE:
        return {"error": "KURE not available"}
    
    # KURE ëª¨ë¸ ë¡œë“œ (í† í¬ë‚˜ì´ì €ë§Œ ì‚¬ìš©)
    try:
        model = SentenceTransformer("nlpai-lab/KURE-v1")
    except:
        try:
            model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        except:
            model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    
    tokenizer = model.tokenizer
    
    results = {
        "name": "KURE",
        "method": "SentenceTransformer ë‚´ì¥ í† í¬ë‚˜ì´ì €",
        "model_name": model.model_name if hasattr(model, 'model_name') else "Unknown",
        "tokens": [],
        "stats": [],
        "processing_time": 0
    }
    
    start_time = time.time()
    
    for text in texts:
        # í† í¬ë‚˜ì´ì§•
        encoded = tokenizer.encode(text, add_special_tokens=False)
        tokens = tokenizer.convert_ids_to_tokens(encoded)
        
        # ì„œë¸Œì›Œë“œ í† í°ì„ ë‹¨ì–´ë¡œ ë³µì› ì‹œë„
        decoded_tokens = []
        current_word = ""
        
        for token in tokens:
            if token.startswith("##") or token.startswith("â–"):
                # ì„œë¸Œì›Œë“œ ì—°ê²°
                if token.startswith("##"):
                    current_word += token[2:]
                else:
                    if current_word:
                        decoded_tokens.append(current_word)
                    current_word = token[1:]
            else:
                if current_word:
                    decoded_tokens.append(current_word)
                current_word = token
        
        if current_word:
            decoded_tokens.append(current_word)
        
        # ì˜ë¯¸ ìˆëŠ” í† í°ë§Œ í•„í„°ë§ (í•œê¸€, ì˜ë¬¸, ìˆ«ì)
        meaningful_tokens = []
        for token in decoded_tokens:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ê¸¸ì´ í™•ì¸
            clean_token = re.sub(r'[^\wê°€-í£]', '', token)
            if len(clean_token) >= 1:  # KUREëŠ” ì„œë¸Œì›Œë“œë¼ ë” ì§§ì„ ìˆ˜ ìˆìŒ
                meaningful_tokens.append(token)
        
        results["tokens"].append({
            "original": text,
            "raw_tokens": tokens[:20],  # ì²˜ìŒ 20ê°œë§Œ í‘œì‹œ
            "decoded_tokens": decoded_tokens,
            "meaningful_tokens": meaningful_tokens,
            "token_count": len(meaningful_tokens),
            "raw_token_count": len(tokens)
        })
        
        results["stats"].append({
            "char_count": len(text),
            "meaningful_tokens": len(meaningful_tokens),
            "raw_tokens": len(tokens),
            "avg_token_length": sum(len(t) for t in meaningful_tokens) / max(len(meaningful_tokens), 1)
        })
    
    results["processing_time"] = time.time() - start_time
    return results

def compare_tokenizers():
    """í† í¬ë‚˜ì´ì € ë¹„êµ ì‹¤í—˜"""
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ë‹¤ì–‘í•œ íŒ¨í„´)
    test_texts = [
        "AI ê¸°ìˆ ì´ ê¸ˆìœµ ì„œë¹„ìŠ¤ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆë‹¤",
        "ì¸í„°ë„·ë±…í‚¹ì—ì„œ 2ë‹¨ê³„ ì¸ì¦ì´ í•„ìˆ˜ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤",
        "ê°€ìƒìì‚° ê±°ë˜ì†Œì˜ ë³´ì•ˆ ê°•í™”ê°€ ì‹œê¸‰í•©ë‹ˆë‹¤",
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ì‚¬ê¸° íƒì§€ ì‹œìŠ¤í…œ",
        "CBDC(Central Bank Digital Currency) ë„ì… ë…¼ì˜",
        "API ë³´ì•ˆê³¼ OAuth 2.0 ì¸ì¦ í”„ë¡œí† ì½œ",
        "í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…ì˜ ê·œì œìƒŒë“œë°•ìŠ¤ ì°¸ì—¬",
        "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ ì´ìš©í•œ ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸ ê°œë°œ"
    ]
    
    print("="*80)
    print("Kiwi vs KURE í† í¬ë‚˜ì´ì € ë¹„êµ ì‹¤í—˜")
    print("="*80)
    
    # Kiwi í…ŒìŠ¤íŠ¸
    print("\n[1] Kiwi í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    kiwi_results = test_kiwi_tokenizer(test_texts)
    
    if "error" not in kiwi_results:
        print(f"ì²˜ë¦¬ ì‹œê°„: {kiwi_results['processing_time']:.4f}ì´ˆ")
        print(f"ë°©ë²•: {kiwi_results['method']}")
        
        for i, result in enumerate(kiwi_results['tokens'][:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\nì˜ˆì‹œ {i+1}: {result['original']}")
            print(f"  êµì •: {result['corrected']}")
            print(f"  ì˜ë¯¸ í† í°: {result['meaningful_tokens']}")
            print(f"  í† í° ìˆ˜: {result['token_count']}")
    else:
        print(f"ì˜¤ë¥˜: {kiwi_results['error']}")
    
    # KURE í…ŒìŠ¤íŠ¸
    print("\n\n[2] KURE í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    kure_results = test_kure_tokenizer(test_texts)
    
    if "error" not in kure_results:
        print(f"ì²˜ë¦¬ ì‹œê°„: {kure_results['processing_time']:.4f}ì´ˆ")
        print(f"ë°©ë²•: {kure_results['method']}")
        print(f"ëª¨ë¸: {kure_results.get('model_name', 'Unknown')}")
        
        for i, result in enumerate(kure_results['tokens'][:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\nì˜ˆì‹œ {i+1}: {result['original']}")
            print(f"  ë””ì½”ë”© í† í°: {result['decoded_tokens']}")
            print(f"  ì˜ë¯¸ í† í°: {result['meaningful_tokens']}")
            print(f"  í† í° ìˆ˜: {result['token_count']}")
    else:
        print(f"ì˜¤ë¥˜: {kure_results['error']}")
    
    # ë¹„êµ ë¶„ì„
    if "error" not in kiwi_results and "error" not in kure_results:
        print("\n\n[3] ë¹„êµ ë¶„ì„")
        print("="*60)
        
        # ì†ë„ ë¹„êµ
        kiwi_time = kiwi_results['processing_time']
        kure_time = kure_results['processing_time'] 
        speed_ratio = kure_time / kiwi_time if kiwi_time > 0 else float('inf')
        
        print(f"ì²˜ë¦¬ ì†ë„:")
        print(f"  Kiwi: {kiwi_time:.4f}ì´ˆ")
        print(f"  KURE: {kure_time:.4f}ì´ˆ")
        print(f"  ë¹„ìœ¨: KUREê°€ Kiwië³´ë‹¤ {speed_ratio:.2f}ë°° {'ë¹ ë¦„' if speed_ratio < 1 else 'ëŠë¦¼'}")
        
        # í† í° ìˆ˜ ë¹„êµ
        kiwi_avg_tokens = sum(s['meaningful_tokens'] for s in kiwi_results['stats']) / len(kiwi_results['stats'])
        kure_avg_tokens = sum(s['meaningful_tokens'] for s in kure_results['stats']) / len(kure_results['stats'])
        
        print(f"\ní‰ê·  í† í° ìˆ˜:")
        print(f"  Kiwi: {kiwi_avg_tokens:.1f}ê°œ")
        print(f"  KURE: {kure_avg_tokens:.1f}ê°œ")
        print(f"  ë¹„ìœ¨: {kure_avg_tokens/kiwi_avg_tokens:.2f}ë°°")
        
        # í† í° ê¸¸ì´ ë¹„êµ
        kiwi_avg_length = sum(s['avg_token_length'] for s in kiwi_results['stats']) / len(kiwi_results['stats'])
        kure_avg_length = sum(s['avg_token_length'] for s in kure_results['stats']) / len(kure_results['stats'])
        
        print(f"\ní‰ê·  í† í° ê¸¸ì´:")
        print(f"  Kiwi: {kiwi_avg_length:.1f}ì")
        print(f"  KURE: {kure_avg_length:.1f}ì")
        
        # ìƒì„¸ ë¹„êµ í…Œì´ë¸”
        print(f"\nìƒì„¸ ë¹„êµ (ì²˜ìŒ 5ê°œ í…ìŠ¤íŠ¸):")
        print(f"{'í…ìŠ¤íŠ¸':<10} {'Kiwií† í°':<8} {'KUREí† í°':<8} {'Kiwiê¸¸ì´':<8} {'KUREê¸¸ì´':<8}")
        print("-" * 50)
        
        for i in range(min(5, len(test_texts))):
            kiwi_stat = kiwi_results['stats'][i]
            kure_stat = kure_results['stats'][i]
            
            print(f"í…ìŠ¤íŠ¸{i+1:<3} {kiwi_stat['meaningful_tokens']:<8} {kure_stat['meaningful_tokens']:<8} "
                  f"{kiwi_stat['avg_token_length']:<8.1f} {kure_stat['avg_token_length']:<8.1f}")
        
        # ì¥ë‹¨ì  ë¶„ì„
        print(f"\nì¥ë‹¨ì  ë¶„ì„:")
        print(f"[Kiwi]")
        print(f"  âœ… í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ì •í™•")
        print(f"  âœ… í•œêµ­ì–´ íŠ¹í™” í’ˆì‚¬ íƒœê¹…")
        print(f"  âœ… ë„ì–´ì“°ê¸° êµì • ê¸°ëŠ¥")
        print(f"  âŒ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦° ì²˜ë¦¬ ì†ë„")
        
        print(f"\n[KURE]")
        print(f"  âœ… ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„")
        print(f"  âœ… ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”©ê³¼ ì¼ê´€ì„±")
        print(f"  âœ… ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ OOV ì²˜ë¦¬")
        print(f"  âŒ í˜•íƒœì†Œ ê²½ê³„ì™€ ë¶ˆì¼ì¹˜ ê°€ëŠ¥")
    
    # ì‚¬ìš© ê¶Œì¥ì‚¬í•­
    print(f"\n\n[4] ì‚¬ìš© ê¶Œì¥ì‚¬í•­")
    print("="*60)
    print(f"ğŸ“Š BM25 í‚¤ì›Œë“œ ê²€ìƒ‰: Kiwi í† í¬ë‚˜ì´ì €")
    print(f"   - í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ì •í™•í•œ ì˜ë¯¸ ë‹¨ìœ„ ì¶”ì¶œ")
    print(f"   - í•œêµ­ì–´ ë¬¸ë²• êµ¬ì¡° ê³ ë ¤")
    print(f"   - ê²€ìƒ‰ ì •í™•ë„ ìš°ì„ ")
    
    print(f"\nğŸ¯ ë²¡í„° ì„ë² ë”© ìƒì„±: KURE í† í¬ë‚˜ì´ì €")
    print(f"   - ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ë™ì¼í•œ í† í¬ë‚˜ì´ì§•")
    print(f"   - ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„")
    print(f"   - ì„ë² ë”© í’ˆì§ˆ ìš°ì„ ")
    
    print(f"\nğŸ”„ í˜„ì¬ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°:")
    print(f"   - BM25: Kiwië¡œ í† í°í™” â†’ í‚¤ì›Œë“œ ë§¤ì¹­")
    print(f"   - Vector: KUREë¡œ ì„ë² ë”© â†’ ìœ ì‚¬ë„ ê³„ì‚°")
    print(f"   - ìµœì ì˜ ì¡°í•©ìœ¼ë¡œ íŒë‹¨ë¨ âœ…")

def detailed_token_analysis():
    """í† í° ë¶„ì„ ìƒì„¸ ë²„ì „"""
    
    analysis_text = "AI ê¸°ìˆ ì„ í™œìš©í•œ í•€í…Œí¬ ì„œë¹„ìŠ¤ê°€ ê¸°ì¡´ ê¸ˆìœµì—…ê³„ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤."
    
    print(f"\n\n[ìƒì„¸ ë¶„ì„] í…ìŠ¤íŠ¸: {analysis_text}")
    print("="*80)
    
    if KIWI_AVAILABLE:
        kiwi = Kiwi()
        
        # Kiwi ìƒì„¸ ë¶„ì„
        print(f"\n[Kiwi ìƒì„¸ ë¶„ì„]")
        corrected = kiwi.space(analysis_text)
        print(f"ë„ì–´ì“°ê¸° êµì •: {corrected}")
        
        morphemes = kiwi.tokenize(analysis_text)
        print(f"\ní˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼:")
        for token in morphemes:
            print(f"  {token.form:<10} {token.tag:<8}")
        
        meaningful = [token.form for token in morphemes 
                     if token.tag.startswith(('N', 'V', 'VA', 'SL')) and len(token.form) >= 2]
        print(f"\nBM25ìš© í† í°: {meaningful}")
    
    if KURE_AVAILABLE:
        try:
            model = SentenceTransformer("nlpai-lab/KURE-v1")
            tokenizer = model.tokenizer
            
            print(f"\n[KURE ìƒì„¸ ë¶„ì„]")
            encoded = tokenizer.encode(analysis_text, add_special_tokens=False)
            tokens = tokenizer.convert_ids_to_tokens(encoded)
            
            print(f"ì„œë¸Œì›Œë“œ í† í°:")
            for i, token in enumerate(tokens):
                print(f"  {i:2d}: {token}")
            
            # í…ìŠ¤íŠ¸ ë³µì›
            decoded = tokenizer.decode(encoded)
            print(f"\në³µì›ëœ í…ìŠ¤íŠ¸: {decoded}")
            
        except Exception as e:
            print(f"KURE ë¶„ì„ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    compare_tokenizers()
    detailed_token_analysis()
    
    print(f"\n" + "="*80)
    print(f"ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
    print(f"="*80)

if __name__ == "__main__":
    main()