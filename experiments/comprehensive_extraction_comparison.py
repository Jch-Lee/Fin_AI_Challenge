#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í¬ê´„ì ì¸ VL vs ê¸°ì¡´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹¤í—˜
ì‹¤ì œ Qwen2.5-VL ëª¨ë¸ê³¼ PyMuPDF ê¸°ë°˜ ê¸°ì¡´ ë°©ë²•ì„ ì§ì ‘ ë¹„êµ
"""

import os
import sys
import time
import json
import torch
import psutil
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import numpy as np
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# ê¸°ì¡´ êµ¬í˜„ import
from packages.preprocessing.pdf_processor_advanced import AdvancedPDFProcessor

# VL ëª¨ë¸ ê´€ë ¨ import
try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    from PIL import Image
    import pymupdf
    import io
    VL_AVAILABLE = True
    print("VL ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    VL_AVAILABLE = False
    print(f"VL ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")

# ============================================================================
# ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================

@dataclass
class ExtractionMetrics:
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë©”íŠ¸ë¦­"""
    method_name: str
    processing_time: float
    peak_memory_mb: float
    char_count: int
    token_count: int
    unique_tokens: int
    chunk_count: int
    visual_elements_detected: int
    error_count: int
    
    def to_dict(self):
        return asdict(self)

@dataclass
class ComparisonResults:
    """ë¹„êµ ì‹¤í—˜ ê²°ê³¼"""
    experiment_id: str
    timestamp: str
    document_path: str
    traditional_metrics: ExtractionMetrics
    vl_metrics: Optional[ExtractionMetrics]
    content_comparison: Dict
    performance_analysis: Dict
    
    def to_dict(self):
        return asdict(self)

# ============================================================================
# ë©”ëª¨ë¦¬ ìµœì í™” VL í”„ë¡œì„¸ì„œ
# ============================================================================

class MemoryOptimizedVLProcessor:
    """ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ì„ ìœ„í•œ VL í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, memory_budget_gb: float = 20.0):
        self.memory_budget = memory_budget_gb * 1024  # MB ë‹¨ìœ„
        self.model = None
        self.processor = None
        self.model_loaded = False
        
    def _get_available_memory(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬ ë°˜í™˜ (MB)"""
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024
            allocated_memory = torch.cuda.memory_allocated(0) / 1024 / 1024
            return total_memory - allocated_memory
        return 0
    
    def _estimate_model_memory(self, quantization_config: Dict) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        base_memory = 14000  # 14GB for FP16
        
        if quantization_config.get("load_in_4bit"):
            return base_memory * 0.25  # ~3.5GB
        elif quantization_config.get("load_in_8bit"):
            return base_memory * 0.5   # ~7GB
        else:
            return base_memory         # ~14GB
    
    def load_model(self) -> bool:
        """ì ì‘í˜• ëª¨ë¸ ë¡œë”©"""
        if not VL_AVAILABLE:
            print("VL ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return False
            
        model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
        available_memory = self._get_available_memory()
        
        # ì–‘ìí™” ì„¤ì • ìš°ì„ ìˆœìœ„
        quantization_configs = [
            {
                "load_in_4bit": True,
                "bnb_4bit_quant_type": "nf4",
                "bnb_4bit_compute_dtype": torch.float16,
                "name": "4-bit"
            },
            {
                "load_in_8bit": True,
                "name": "8-bit"
            },
            {
                "torch_dtype": torch.float16,
                "name": "FP16"
            }
        ]
        
        for config in quantization_configs:
            estimated_memory = self._estimate_model_memory(config)
            
            print(f"ì‹œë„: {config['name']} ì–‘ìí™” (ì˜ˆìƒ ë©”ëª¨ë¦¬: {estimated_memory:.0f}MB)")
            
            if estimated_memory < available_memory:
                try:
                    # BitsAndBytesConfig ì„¤ì •
                    if config.get("load_in_4bit") or config.get("load_in_8bit"):
                        from transformers import BitsAndBytesConfig
                        bnb_config = BitsAndBytesConfig(**{k: v for k, v in config.items() if k != "name"})
                        
                        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                            model_name,
                            quantization_config=bnb_config,
                            device_map="auto",
                            trust_remote_code=True
                        )
                    else:
                        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                            model_name,
                            torch_dtype=config.get("torch_dtype", torch.float16),
                            device_map="auto",
                            trust_remote_code=True
                        )
                    
                    self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
                    self.model_loaded = True
                    
                    actual_memory = torch.cuda.memory_allocated(0) / 1024 / 1024
                    print(f"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ: {config['name']} (ì‹¤ì œ ë©”ëª¨ë¦¬: {actual_memory:.0f}MB)")
                    return True
                    
                except Exception as e:
                    print(f"âŒ {config['name']} ë¡œë”© ì‹¤íŒ¨: {e}")
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    continue
            else:
                print(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ {config['name']} ìŠ¤í‚µ")
        
        print("âŒ ëª¨ë“  ì–‘ìí™” ì„¤ì • ì‹¤íŒ¨")
        return False
    
    def process_pdf_pages(self, pdf_path: str, max_pages: int = 10) -> List[str]:
        """PDF í˜ì´ì§€ë¥¼ VL ëª¨ë¸ë¡œ ì²˜ë¦¬"""
        if not self.model_loaded:
            print("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return []
        
        results = []
        doc = pymupdf.open(pdf_path)
        
        try:
            for page_num in range(min(max_pages, len(doc))):
                print(f"í˜ì´ì§€ {page_num + 1}/{min(max_pages, len(doc))} ì²˜ë¦¬ ì¤‘...")
                
                page = doc[page_num]
                
                # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                pix = page.get_pixmap(dpi=150)
                img_data = pix.tobytes("png")
                image = Image.open(io.BytesIO(img_data))
                
                # VL ëª¨ë¸ í”„ë¡¬í”„íŠ¸
                prompt = """ì´ ë¬¸ì„œ í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ì™€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
                
ë‹¤ìŒì— ì£¼ì˜í•˜ì„¸ìš”:
- ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”
- ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ê°€ ìˆë‹¤ë©´ ë°ì´í„° í¬ì¸íŠ¸ì™€ ìˆ˜ì¹˜ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”
- í…Œì´ë¸”ì´ ìˆë‹¤ë©´ ëª¨ë“  ì…€ ë‚´ìš©ì„ ì½ì–´ì£¼ì„¸ìš”
- ë‹¤ì´ì–´ê·¸ë¨ì´ ìˆë‹¤ë©´ í…ìŠ¤íŠ¸ì™€ ì—°ê²° ê´€ê³„ë¥¼ ì„¤ëª…í•˜ì„¸ìš”

í˜•íƒœë‚˜ êµ¬ì¡° ì„¤ëª…ë³´ë‹¤ëŠ” ì‹¤ì œ ë‚´ìš©ê³¼ ë°ì´í„°ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”."""
                
                # ë©”ì‹œì§€ êµ¬ì„±
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }]
                
                try:
                    # ëª¨ë¸ ì¶”ë¡ 
                    text = self.processor.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    inputs = self.processor(
                        text=[text],
                        images=[image],
                        padding=True,
                        return_tensors="pt"
                    )
                    
                    # GPUë¡œ ì´ë™
                    inputs = {k: v.to(self.model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                    
                    # ìƒì„±
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=1024,
                            temperature=0.1,
                            do_sample=False,
                            pad_token_id=self.processor.tokenizer.eos_token_id
                        )
                    
                    # ë””ì½”ë”©
                    generated_ids = [
                        output_ids[len(input_ids):] 
                        for input_ids, output_ids in zip(inputs['input_ids'], outputs)
                    ]
                    
                    response = self.processor.batch_decode(
                        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
                    )[0]
                    
                    results.append(response.strip())
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del inputs, outputs, generated_ids
                    torch.cuda.empty_cache()
                    
                except Exception as e:
                    print(f"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    results.append(f"[í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}]")
        
        finally:
            doc.close()
        
        return results
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if self.processor is not None:
            del self.processor
            self.processor = None
        
        self.model_loaded = False
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")

# ============================================================================
# ê¸°ì¡´ ë°©ë²• ì¶”ì¶œê¸°
# ============================================================================

class TraditionalExtractor:
    """ê¸°ì¡´ PyMuPDF ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    
    def __init__(self):
        self.pdf_processor = AdvancedPDFProcessor()
    
    def extract_from_pdf(self, pdf_path: str) -> str:
        """ê¸°ì¡´ ë°©ë²•ìœ¼ë¡œ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ê¸°ì¡´ AdvancedPDFProcessor ì‚¬ìš©
            result = self.pdf_processor.extract_pdf(pdf_path)
            
            # ë§ˆí¬ë‹¤ìš´ê³¼ í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©
            combined_text = []
            
            if result.markdown:
                combined_text.append("=== MARKDOWN ì¶”ì¶œ ê²°ê³¼ ===")
                combined_text.append(result.markdown)
            
            if result.page_texts:
                combined_text.append("=== í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ===")
                for i, page_text in enumerate(result.page_texts):
                    if page_text.strip():
                        combined_text.append(f"[í˜ì´ì§€ {i+1}]")
                        combined_text.append(page_text)
            
            return "\n\n".join(combined_text)
        
        except Exception as e:
            print(f"ê¸°ì¡´ ë°©ë²• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return f"[ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}]"

# ============================================================================
# ì‹¤í—˜ ì‹¤í–‰ê¸°
# ============================================================================

class ExtractionComparisonExperiment:
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹¤í—˜"""
    
    def __init__(self, output_dir: str = "experiments/results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.traditional_extractor = TraditionalExtractor()
        self.vl_processor = MemoryOptimizedVLProcessor()
        
    def _measure_performance(self, func, *args, **kwargs) -> Tuple[any, float, float]:
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            start_gpu_memory = torch.cuda.memory_allocated(0) / 1024 / 1024
        else:
            start_gpu_memory = 0
        
        # í•¨ìˆ˜ ì‹¤í–‰
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
        end_memory = process.memory_info().rss / 1024 / 1024
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            end_gpu_memory = torch.cuda.memory_allocated(0) / 1024 / 1024
        else:
            end_gpu_memory = 0
        
        execution_time = end_time - start_time
        peak_memory = max(end_memory - start_memory, end_gpu_memory - start_gpu_memory)
        
        return result, execution_time, peak_memory
    
    def _analyze_content(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¶„ì„"""
        words = text.split()
        
        return {
            "char_count": len(text),
            "word_count": len(words),
            "unique_words": len(set(words)),
            "lines": len(text.split('\n')),
            "visual_keywords": sum(1 for word in words if word.lower() in 
                                 ['ê·¸ë¦¼', 'í‘œ', 'ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ë„í‘œ', 'figure', 'table', 'chart']),
            "financial_keywords": sum(1 for word in words if word.lower() in
                                    ['ê¸ˆìœµ', 'ë³´ì•ˆ', 'ìœ„í—˜', 'ê·œì œ', 'ì»´í”Œë¼ì´ì–¸ìŠ¤', 'ë¦¬ìŠ¤í¬'])
        }
    
    def run_traditional_extraction(self, pdf_path: str) -> ExtractionMetrics:
        """ê¸°ì¡´ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰"""
        print("\n=== ê¸°ì¡´ ë°©ë²• í…ìŠ¤íŠ¸ ì¶”ì¶œ ===")
        
        result, exec_time, peak_mem = self._measure_performance(
            self.traditional_extractor.extract_from_pdf, pdf_path
        )
        
        # ê²°ê³¼ ì €ì¥
        output_file = self.output_dir / "traditional_extraction_output.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result)
        
        # ì½˜í…ì¸  ë¶„ì„
        content_analysis = self._analyze_content(result)
        
        metrics = ExtractionMetrics(
            method_name="Traditional_PyMuPDF",
            processing_time=exec_time,
            peak_memory_mb=peak_mem,
            char_count=content_analysis["char_count"],
            token_count=content_analysis["word_count"],
            unique_tokens=content_analysis["unique_words"],
            chunk_count=content_analysis["lines"],
            visual_elements_detected=content_analysis["visual_keywords"],
            error_count=1 if "ì¶”ì¶œ ì‹¤íŒ¨" in result else 0
        )
        
        print(f"ì²˜ë¦¬ ì‹œê°„: {exec_time:.2f}ì´ˆ")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©: {peak_mem:.0f}MB")
        print(f"ì¶”ì¶œ ë¬¸ì ìˆ˜: {metrics.char_count:,}")
        print(f"ê²°ê³¼ ì €ì¥: {output_file}")
        
        return metrics
    
    def run_vl_extraction(self, pdf_path: str, max_pages: int = 10) -> Optional[ExtractionMetrics]:
        """VL ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰"""
        print("\n=== VL ëª¨ë¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ===")
        
        # ëª¨ë¸ ë¡œë”©
        print("VL ëª¨ë¸ ë¡œë”© ì¤‘...")
        if not self.vl_processor.load_model():
            print("VL ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            return None
        
        try:
            # VL ì¶”ì¶œ ì‹¤í–‰
            result, exec_time, peak_mem = self._measure_performance(
                self.vl_processor.process_pdf_pages, pdf_path, max_pages
            )
            
            # ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            combined_text = "\n\n".join([f"[í˜ì´ì§€ {i+1}]\n{content}" for i, content in enumerate(result)])
            
            # ê²°ê³¼ ì €ì¥
            output_file = self.output_dir / "vl_extraction_output.txt"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(combined_text)
            
            # ì½˜í…ì¸  ë¶„ì„
            content_analysis = self._analyze_content(combined_text)
            
            metrics = ExtractionMetrics(
                method_name="VL_Qwen2.5-VL-7B",
                processing_time=exec_time,
                peak_memory_mb=peak_mem,
                char_count=content_analysis["char_count"],
                token_count=content_analysis["word_count"],
                unique_tokens=content_analysis["unique_words"],
                chunk_count=len(result),
                visual_elements_detected=content_analysis["visual_keywords"],
                error_count=sum(1 for r in result if "ì²˜ë¦¬ ì‹¤íŒ¨" in r)
            )
            
            print(f"ì²˜ë¦¬ ì‹œê°„: {exec_time:.2f}ì´ˆ")
            print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©: {peak_mem:.0f}MB")
            print(f"ì¶”ì¶œ ë¬¸ì ìˆ˜: {metrics.char_count:,}")
            print(f"ì²˜ë¦¬ í˜ì´ì§€: {len(result)}ê°œ")
            print(f"ê²°ê³¼ ì €ì¥: {output_file}")
            
            return metrics
        
        finally:
            # ëª¨ë¸ ì–¸ë¡œë“œ
            self.vl_processor.unload_model()
    
    def compare_methods(self, pdf_path: str, max_pages: int = 10) -> ComparisonResults:
        """ë‘ ë°©ë²• ë¹„êµ ì‹¤í–‰"""
        print(f"\nğŸ“„ ë¬¸ì„œ ë¶„ì„: {pdf_path}")
        print("=" * 60)
        
        experiment_id = hashlib.md5(f"{pdf_path}_{datetime.now()}".encode()).hexdigest()[:8]
        
        # 1. ê¸°ì¡´ ë°©ë²• ì‹¤í–‰
        traditional_metrics = self.run_traditional_extraction(pdf_path)
        
        # 2. VL ë°©ë²• ì‹¤í–‰
        vl_metrics = self.run_vl_extraction(pdf_path, max_pages)
        
        # 3. ë¹„êµ ë¶„ì„
        content_comparison = self._compare_content(traditional_metrics, vl_metrics)
        performance_analysis = self._analyze_performance(traditional_metrics, vl_metrics)
        
        # 4. ê²°ê³¼ êµ¬ì„±
        results = ComparisonResults(
            experiment_id=experiment_id,
            timestamp=datetime.now().isoformat(),
            document_path=pdf_path,
            traditional_metrics=traditional_metrics,
            vl_metrics=vl_metrics,
            content_comparison=content_comparison,
            performance_analysis=performance_analysis
        )
        
        # 5. ê²°ê³¼ ì €ì¥
        self._save_results(results)
        
        return results
    
    def _compare_content(self, traditional: ExtractionMetrics, vl: Optional[ExtractionMetrics]) -> Dict:
        """ì½˜í…ì¸  ë¹„êµ ë¶„ì„"""
        if vl is None:
            return {
                "comparison_possible": False,
                "reason": "VL extraction failed"
            }
        
        comparison = {
            "comparison_possible": True,
            "content_increase": {
                "char_count": ((vl.char_count - traditional.char_count) / traditional.char_count * 100) if traditional.char_count > 0 else 0,
                "token_count": ((vl.token_count - traditional.token_count) / traditional.token_count * 100) if traditional.token_count > 0 else 0,
                "unique_tokens": ((vl.unique_tokens - traditional.unique_tokens) / traditional.unique_tokens * 100) if traditional.unique_tokens > 0 else 0
            },
            "visual_elements": {
                "traditional": traditional.visual_elements_detected,
                "vl": vl.visual_elements_detected,
                "improvement": vl.visual_elements_detected - traditional.visual_elements_detected
            },
            "error_comparison": {
                "traditional_errors": traditional.error_count,
                "vl_errors": vl.error_count
            }
        }
        
        return comparison
    
    def _analyze_performance(self, traditional: ExtractionMetrics, vl: Optional[ExtractionMetrics]) -> Dict:
        """ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
        if vl is None:
            return {
                "analysis_possible": False,
                "reason": "VL extraction failed"
            }
        
        analysis = {
            "analysis_possible": True,
            "processing_time": {
                "traditional": traditional.processing_time,
                "vl": vl.processing_time,
                "ratio": vl.processing_time / traditional.processing_time if traditional.processing_time > 0 else float('inf')
            },
            "memory_usage": {
                "traditional": traditional.peak_memory_mb,
                "vl": vl.peak_memory_mb,
                "additional_memory": vl.peak_memory_mb - traditional.peak_memory_mb
            },
            "efficiency": {
                "traditional_chars_per_sec": traditional.char_count / traditional.processing_time if traditional.processing_time > 0 else 0,
                "vl_chars_per_sec": vl.char_count / vl.processing_time if vl.processing_time > 0 else 0
            }
        }
        
        return analysis
    
    def _save_results(self, results: ComparisonResults):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        # JSON ê²°ê³¼ ì €ì¥
        json_file = self.output_dir / f"comparison_results_{results.experiment_id}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results.to_dict(), f, indent=2, ensure_ascii=False)
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_summary_report(results)
        
        print(f"\nğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  - JSON ê²°ê³¼: {json_file}")
        print(f"  - ìš”ì•½ ë¦¬í¬íŠ¸: {self.output_dir}/comparison_summary.md")
    
    def _generate_summary_report(self, results: ComparisonResults):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("# í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹¤í—˜ ê²°ê³¼\n")
        report.append(f"**ì‹¤í—˜ ID**: {results.experiment_id}")
        report.append(f"**ì‹¤í—˜ ì¼ì‹œ**: {results.timestamp}")
        report.append(f"**ë¬¸ì„œ**: {Path(results.document_path).name}\n")
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë¹„êµ
        report.append("## ğŸ“Š ê¸°ë³¸ ë©”íŠ¸ë¦­ ë¹„êµ\n")
        report.append("| ë©”íŠ¸ë¦­ | ê¸°ì¡´ ë°©ë²• | VL ë°©ë²• | ê°œì„ ìœ¨ |")
        report.append("|--------|-----------|---------|--------|")
        
        trad = results.traditional_metrics
        vl = results.vl_metrics
        
        if vl:
            comp = results.content_comparison
            report.append(f"| ì¶”ì¶œ ë¬¸ì ìˆ˜ | {trad.char_count:,} | {vl.char_count:,} | {comp['content_increase']['char_count']:+.1f}% |")
            report.append(f"| í† í° ìˆ˜ | {trad.token_count:,} | {vl.token_count:,} | {comp['content_increase']['token_count']:+.1f}% |")
            report.append(f"| ê³ ìœ  í† í° | {trad.unique_tokens:,} | {vl.unique_tokens:,} | {comp['content_increase']['unique_tokens']:+.1f}% |")
            report.append(f"| ì²˜ë¦¬ ì‹œê°„ | {trad.processing_time:.2f}ì´ˆ | {vl.processing_time:.2f}ì´ˆ | {((vl.processing_time/trad.processing_time-1)*100):+.1f}% |")
            report.append(f"| ë©”ëª¨ë¦¬ ì‚¬ìš© | {trad.peak_memory_mb:.0f}MB | {vl.peak_memory_mb:.0f}MB | +{vl.peak_memory_mb-trad.peak_memory_mb:.0f}MB |")
        else:
            report.append(f"| ì¶”ì¶œ ë¬¸ì ìˆ˜ | {trad.char_count:,} | N/A | N/A |")
            report.append(f"| í† í° ìˆ˜ | {trad.token_count:,} | N/A | N/A |")
            report.append(f"| ì²˜ë¦¬ ì‹œê°„ | {trad.processing_time:.2f}ì´ˆ | N/A | N/A |")
        
        # ì„±ëŠ¥ ë¶„ì„
        if vl and results.performance_analysis.get("analysis_possible"):
            perf = results.performance_analysis
            report.append("\n## âš¡ ì„±ëŠ¥ ë¶„ì„\n")
            report.append(f"- **ì²˜ë¦¬ ì†ë„ ë¹„ìœ¨**: VL ë°©ë²•ì´ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ {perf['processing_time']['ratio']:.1f}ë°°")
            report.append(f"- **ì²˜ë¦¬ íš¨ìœ¨ì„±**: ê¸°ì¡´ {perf['efficiency']['traditional_chars_per_sec']:.0f} vs VL {perf['efficiency']['vl_chars_per_sec']:.0f} ë¬¸ì/ì´ˆ")
            report.append(f"- **ì¶”ê°€ ë©”ëª¨ë¦¬**: {perf['memory_usage']['additional_memory']:.0f}MB")
        
        # ì‹œê°ì  ìš”ì†Œ ë¶„ì„
        if vl and results.content_comparison.get("comparison_possible"):
            visual = results.content_comparison["visual_elements"]
            report.append("\n## ğŸ‘ï¸ ì‹œê°ì  ìš”ì†Œ ê°ì§€\n")
            report.append(f"- **ê¸°ì¡´ ë°©ë²•**: {visual['traditional']}ê°œ")
            report.append(f"- **VL ë°©ë²•**: {visual['vl']}ê°œ")
            report.append(f"- **ê°œì„ **: {visual['improvement']:+d}ê°œ")
        
        # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
        report.append("\n## ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n")
        
        if vl:
            if results.content_comparison["content_increase"]["char_count"] > 10:
                report.append("âœ… **VL í†µí•© ê¶Œì¥**: ì½˜í…ì¸  ì¶”ì¶œëŸ‰ì´ ìœ ì˜ë¯¸í•˜ê²Œ ì¦ê°€")
            elif results.content_comparison["visual_elements"]["improvement"] > 5:
                report.append("âœ… **VL í†µí•© ê¶Œì¥**: ì‹œê°ì  ìš”ì†Œ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ")
            else:
                report.append("âš ï¸ **ì‹ ì¤‘í•œ ê²€í†  í•„ìš”**: ì„±ëŠ¥ í–¥ìƒì´ ì œí•œì ")
                
            if vl.peak_memory_mb > 20000:  # 20GB
                report.append("âš ï¸ **ë©”ëª¨ë¦¬ ìµœì í™” í•„ìš”**: ëŒ€íšŒ í™˜ê²½ ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤")
                
            if vl.processing_time > trad.processing_time * 3:
                report.append("âš ï¸ **ì²˜ë¦¬ ì‹œê°„ ìµœì í™” í•„ìš”**: ëŒ€íšŒ ì‹œê°„ ì œí•œ ê³ ë ¤")
        else:
            report.append("âŒ **VL í†µí•© ë¶ˆê°€**: ëª¨ë¸ ë¡œë”© ë˜ëŠ” ì²˜ë¦¬ ì‹¤íŒ¨")
            report.append("ğŸ“ **ê¶Œì¥ì‚¬í•­**: ë©”ëª¨ë¦¬ ì œì•½ ì™„í™” ë˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™” ê²€í† ")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = self.output_dir / "comparison_summary.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    print("=" * 60)
    print("ğŸ” í¬ê´„ì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹¤í—˜")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì°¾ê¸°
    test_documents = [
        "data/raw/ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf",
        "docs/ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf",
        "data/documents/ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf",
    ]
    
    pdf_path = None
    for doc_path in test_documents:
        if Path(doc_path).exists():
            pdf_path = doc_path
            break
    
    if not pdf_path:
        print("âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— PDF ë¬¸ì„œë¥¼ ë°°ì¹˜í•˜ì„¸ìš”:")
        for doc_path in test_documents:
            print(f"  - {doc_path}")
        return
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiment = ExtractionComparisonExperiment()
    
    try:
        results = experiment.compare_methods(pdf_path, max_pages=10)
        
        print("\n" + "=" * 60)
        print("âœ… ì‹¤í—˜ ì™„ë£Œ!")
        print("=" * 60)
        
        if results.vl_metrics:
            print(f"ğŸ“ˆ ì½˜í…ì¸  ì¦ê°€: {results.content_comparison['content_increase']['char_count']:+.1f}%")
            print(f"âš¡ ì²˜ë¦¬ ì‹œê°„ ë¹„ìœ¨: {results.performance_analysis['processing_time']['ratio']:.1f}x")
            print(f"ğŸ’¾ ì¶”ê°€ ë©”ëª¨ë¦¬: {results.performance_analysis['memory_usage']['additional_memory']:.0f}MB")
        else:
            print("âš ï¸ VL ë°©ë²• ì‹¤í–‰ ì‹¤íŒ¨ - ê¸°ì¡´ ë°©ë²• ê²°ê³¼ë§Œ ì €ì¥ë¨")
        
        print(f"\nğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {experiment.output_dir}")
        
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì‹¤í—˜ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()