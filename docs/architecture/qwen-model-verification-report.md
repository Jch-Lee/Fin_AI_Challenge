# Qwen 모델 검증 보고서

**검증 일시**: 2025-08-12
**검증 방법**: Hugging Face 공식 모델 카드 확인

## 검증된 모델 목록

### 1. Qwen2.5-1.5B-Instruct
- **공식 리포지토리**: `Qwen/Qwen2.5-1.5B-Instruct`
- **출시일**: 2024년 9월 19일 (대회 규칙 적합 ✅)
- **라이선스**: Apache 2.0 (상업적 이용 가능 ✅)
- **파라미터**: 1.54B (1.31B non-embedding)
- **아키텍처**: Transformer with RoPE, SwiGLU, RMSNorm, QKV bias
- **컨텍스트 길이**: 32,768 tokens
- **생성 길이**: 최대 8,192 tokens
- **다국어 지원**: 29개 이상 언어 (한국어 포함)
- **특징**: 구조화된 데이터 이해 개선, JSON 출력 생성 능력 향상
- **시스템 요구사항**: transformers>=4.37.0

### 2. Qwen2.5-7B-Instruct
- **공식 리포지토리**: `Qwen/Qwen2.5-7B-Instruct`
- **출시일**: 2024년 9월 19일 (대회 규칙 적합 ✅)
- **라이선스**: Apache 2.0 (상업적 이용 가능 ✅)
- **파라미터**: 7.61B (6.53B non-embedding)
- **아키텍처**: Transformer with RoPE, SwiGLU, RMSNorm, QKV bias
- **컨텍스트 길이**: 131,072 tokens
- **생성 길이**: 최대 8,192 tokens
- **다국어 지원**: 29개 이상 언어 (한국어 포함)
- **특징**: 장문 생성, 수학/코딩 능력 강화, 128K 컨텍스트 지원
- **시스템 요구사항**: transformers>=4.37.0
- **최적화**: vLLM 지원

### 3. Qwen2.5-14B-Instruct
- **공식 리포지토리**: `Qwen/Qwen2.5-14B-Instruct`
- **출시일**: 2024년 9월 19일 (대회 규칙 적합 ✅)
- **라이선스**: Apache 2.0 (상업적 이용 가능 ✅)
- **파라미터**: 14.7B (13.1B non-embedding)
- **아키텍처**: Transformer with RoPE, SwiGLU, RMSNorm
- **레이어**: 48개
- **어텐션 헤드**: 40 Q heads, 8 KV heads
- **컨텍스트 길이**: 131,072 tokens
- **생성 길이**: 최대 8,192 tokens
- **다국어 지원**: 29개 이상 언어 (한국어 포함)
- **특징**: YaRN 기법으로 긴 텍스트 처리, vLLM 최적화
- **시스템 요구사항**: transformers>=4.37.0

## 부적격 모델

### Qwen3-30B-A3B-Instruct-2507
- **공식 리포지토리**: `Qwen/Qwen3-30B-A3B-Instruct-2507`
- **출시일**: 2025년 7월 30일 (대회 규칙 위반 ❌)
- **부적격 사유**: 2025년 8월 1일 이후 출시
- **라이선스**: Apache 2.0
- **파라미터**: 30.5B (MoE, 3.3B active per inference)
- **전문가**: 128개 중 8개 활성화
- **컨텍스트 길이**: 262,144 tokens (최대 1M tokens 지원)
- **특징**: Non-thinking mode, Dual Chunk Attention, MInference 기법

## Qwen 2.5 시리즈 주요 개선사항

1. **지식 확장**: Qwen2 대비 대폭 확장된 지식 베이스
2. **코딩/수학 능력**: 전문화된 모델을 통한 능력 강화
3. **명령 추종**: 명령 수행 능력 대폭 개선
4. **장문 생성**: 8K 토큰 이상 생성 능력
5. **구조화 데이터**: 테이블 등 구조화 데이터 이해 개선
6. **구조화 출력**: JSON 등 구조화 출력 생성 개선
7. **긴 컨텍스트**: 128K 토큰 지원 및 생성
8. **다국어**: 29개 이상 언어 지원 (한국어 포함)

## 훈련 데이터 정보

- **데이터셋 크기**: 최대 18조 토큰
- **라이선스**: Apache 2.0 (3B, 72B 제외)
- **통합**: Hugging Face transformers 라이브러리 공식 지원

## 권장사항

1. **Student Model**: Qwen2.5-1.5B-Instruct 사용 권장
   - 경량화되어 있어 4-bit 양자화 시 메모리 효율적
   - 한국어 지원 우수
   - 구조화된 출력 생성에 적합

2. **Teacher Model**: Qwen2.5-7B-Instruct 또는 Qwen2.5-14B-Instruct
   - 강력한 명령 수행 능력
   - 긴 컨텍스트 처리 가능
   - 수학/논리적 추론 능력 우수

3. **메모리 고려사항**: 
   - RTX 4090 24GB 기준으로 14B 모델도 4-bit 양자화로 추론 가능
   - vLLM 최적화 활용 권장

## 검증 결과 요약

- **총 검증 모델**: 4개 (3개 적격, 1개 부적격)
- **적격 모델**: Qwen2.5-1.5B/7B/14B-Instruct (모두 2024-09-19 출시)
- **부적격 모델**: Qwen3-30B-A3B-Instruct-2507 (2025-07-30 출시)
- **라이선스**: 모든 적격 모델이 Apache 2.0 라이선스
- **기술 요구사항**: transformers>=4.37.0, vLLM 지원

---
*검증 기준: 대회 규칙 "2025년 8월 1일 이전에 공개된 모델" 및 "비상업적 이용 허용 라이선스"*