# 3.2. 추론 파이프라인 구축

## 구현 현황 (2025-08-23 최종 업데이트)

### ✅ 완료된 컴포넌트

#### 3.2.1. Question Classifier 컴포넌트 (구현 완료)
**위치**: 
- `scripts/generate_submission_remote_8bit_fixed.py::is_multiple_choice()`
- `scripts/generate_submission_standalone.py::is_multiple_choice()`

**기능**:
- 객관식/주관식 자동 분류
- 숫자로 시작하는 선택지 2개 이상 감지
- 질문과 선택지 분리

**구현 코드**:
```python
def is_multiple_choice(self, question: str) -> bool:
    lines = question.split('\n')
    choices = []
    for line in lines:
        line = line.strip()
        if line and line[0].isdigit() and len(line) > 1:
            if '.' in line[:3] or ')' in line[:3] or ' ' in line[:3]:
                choices.append(line)
    return len(choices) >= 2
```

#### 3.2.3. Multi-Stage Retriever 컴포넌트 (구현 완료)
**위치**: 
- `scripts/generate_submission_remote_8bit_fixed.py::retrieve_combined_contexts()`
- `scripts/generate_submission_standalone.py::retrieve_combined_contexts()`

**BM25 + FAISS 하이브리드 검색**:
- **BM25 검색**: 
  - Kiwi 토크나이저 기반 희소 검색
  - `get_top_n()` 메서드 사용 (get_scores 대신)
  - 상위 3개 문서 선택
- **Vector 검색**: 
  - KURE-v1 임베딩 (nlpai-lab/KURE-v1, 1024차원)
  - FAISS 코사인 유사도 기반 밀집 검색
  - 상위 3개 문서 선택
- **독립 선택**: 각 방법에서 독립적으로 3개씩 선택
- **총 6개 컨텍스트**: 중복 허용으로 정보 다양성 극대화

**구현 코드**:
```python
def retrieve_combined_contexts(self, question: str) -> List[str]:
    # BM25 상위 3개 검색 (get_top_n 사용)
    bm25_contexts = self.search_bm25(question, k=3)
    
    # Vector 상위 3개 검색 (FAISS 사용)
    vector_contexts = self.search_vector(question, k=3)
    
    # 결합 (중복 허용으로 총 6개)
    combined_contexts = bm25_contexts + vector_contexts
    
    return combined_contexts
```

#### 3.2.4. Inference Orchestrator 컴포넌트 (구현 완료)
**위치**: 
- `scripts/generate_submission_remote_8bit_fixed.py::QwenUpdatedDBPredictor`
- `scripts/generate_submission_standalone.py::QwenStandalonePredictor`

**전체 추론 흐름**:
```
1. 시스템 초기화 (setup)
   - 8,756개 청크 로드 (chunks_2300.json)
   - BM25 인덱스 로드 (bm25_index_2300.pkl)
   - FAISS 인덱스 로드 (faiss_index_2300.index)
   - KURE-v1 임베더 초기화 (nlpai-lab/KURE-v1)
   - Qwen2.5-7B-Instruct 8-bit 모델 로드

2. 질문별 처리 (predict)
   - 질문 분류 (is_multiple_choice)
   - 하이브리드 검색 (retrieve_combined_contexts)
     • BM25: get_top_n() → Top-3
     • Vector: FAISS search() → Top-3
   - 프롬프트 생성 (create_prompt)
     • Chat Template 형식 사용
     • 참고 문서 전체 사용 (길이 제한 제거)
   - 8-bit 모델 추론 (generate_answer)
     • 객관식: max_new_tokens=32
     • 주관식: max_new_tokens=256
   - 답변 후처리 (extract_answer)
     • 이미지/URL 패턴 제거
     • 최대 500자 제한

3. 최적화
   - 10개마다 메모리 정리 (torch.cuda.empty_cache)
   - 이미지 토큰 차단 (bad_words_ids)
   - 결정론적 생성 (do_sample=False)
   - 한국어 최적화 (Kiwi + KURE-v1)
```

### ⏳ 미구현 컴포넌트

#### 3.2.2. Cache Layer 컴포넌트 (계획됨)
**예상 구현**:
```python
# diskcache 또는 Redis 기반 캐싱
class CacheLayer:
    def __init__(self):
        self.cache = diskcache.Cache('./cache')
    
    def get_or_compute(self, key, compute_fn):
        if key in self.cache:
            return self.cache[key]
        result = compute_fn()
        self.cache[key] = result
        return result
```

**캐싱 대상**:
- 자주 나오는 질문의 답변
- 임베딩 결과
- 검색 결과

## 실제 구현 아키텍처

### 8-bit 양자화 추론 파이프라인

```
질문 입력 (test.csv)
    ↓
Question Classifier
    ├── 객관식: 선택지 추출
    └── 주관식: 전체 질문 처리
    ↓
Multi-Stage Retriever
    ├── BM25 검색 (Top-3)
    │   └── Kiwi 토크나이저
    └── Vector 검색 (Top-3)
        └── KURE-v1 임베딩
    ↓
컨텍스트 결합 (6개)
    ↓
프롬프트 생성
    ├── 객관식: "숫자만 출력"
    └── 주관식: "한국어로 설명"
    ↓
Qwen2.5-7B-Instruct (8-bit)
    ├── temperature: 0.05
    ├── top_p: 0.9, top_k: 5
    └── bad_words_ids: 이미지 차단
    ↓
답변 후처리
    ├── 이미지 링크 제거
    ├── 한국어 외 문자 제거
    └── 길이 제한 (500자)
    ↓
제출 파일 생성 (CSV)
```

## 핵심 성능 지표

### 처리 성능 (실측값)
- **속도**: 5.75초/질문 (약 0.17 문제/초)
- **메모리**: 10.7GB VRAM (8-bit 양자화)
- **전체 시간**: 515개 문제 약 49분 예상
- **대회 제한**: 4.5시간 (270분) - 5.5배 여유

### 검색 품질
- **BM25**: 키워드 정확 매칭 (Kiwi 형태소 분석 기반)
- **Vector**: 의미적 유사성 (KURE-v1 1024차원 임베딩)
- **하이브리드**: 정보 다양성 극대화 (각 3개씩 독립 선택)

### 메모리 최적화
- **8-bit 양자화**: 약 55% 메모리 절감 (24GB → 10.7GB)
- **가비지 컬렉션**: 10개마다 실행 (torch.cuda.empty_cache)
- **RTX 4090 충족**: 24GB VRAM 내 안정적 동작

## 사용 라이브러리

```python
# 모델 및 양자화
torch==2.1.0                    # PyTorch (대회 요구사항)
transformers==4.41.2            # Hugging Face Transformers
bitsandbytes==0.43.1            # 8-bit 양자화
accelerate==0.30.1              # GPU 가속

# 검색 시스템
faiss-cpu==1.8.0                # 벡터 검색 엔진
rank-bm25==0.2.2                # BM25 검색
sentence-transformers==2.7.0    # KURE-v1 임베더

# 한국어 처리
kiwipiepy==0.18.0               # 형태소 분석기

# 데이터 처리
pandas==2.2.2                   # CSV 처리
numpy==1.26.4                   # 수치 연산
```

## 실행 방법

### 원격 서버용 (generate_submission_remote_8bit_fixed.py)
```bash
# 빠른 테스트 (10개)
python scripts/generate_submission_remote_8bit_fixed.py --quick-test

# 전체 추론
python scripts/generate_submission_remote_8bit_fixed.py

# 특정 테스트 파일
python scripts/generate_submission_remote_8bit_fixed.py --test-file custom_test.csv
```

### 독립 실행 버전 (generate_submission_standalone.py)
```bash
# 테스트 모드 (10개)
python scripts/generate_submission_standalone.py \
    --test_mode \
    --num_samples 10 \
    --output_file test_result.csv

# 전체 추론 (515개)
python scripts/generate_submission_standalone.py \
    --input_file test.csv \
    --output_file submission.csv \
    --data_dir /workspace/Fin_AI_Challenge/data/rag
```

## 다음 단계

1. **Cache Layer 구현**
   - 중복 질문 캐싱
   - 임베딩 결과 재사용

2. **성능 개선**
   - 배치 처리 최적화
   - 병렬 처리 도입

3. **Teacher-Student 준비**
   - 3,000개 합성 데이터 활용
   - Distillation 파이프라인 구축

---
*Last Updated: 2025-08-23 - 8-bit 양자화 추론 파이프라인 구현 완료 및 검증*