# 3. 과제 명세 (Task Specification) - 상세 버전

## ### 3.1 입력 (Input)

- **파일 형식:** `test.csv`, UTF-8 인코딩된 CSV 파일.
- **컬럼 구조:**
    - `id`: 각 질문의 고유 식별자 (예: `TEST_0000`).
    - `질문`: 실제 과제 내용이 담긴 텍스트.
- **질문 유형 분석:** `질문` 컬럼에는 최소 두 가지 유형의 문제가 포함되어 있습니다.
    - **객관식 (Multiple Choice):** "다음 중 ... 에 해당하는 것을 고르시오." 와 같은 형식으로, 보기(1., 2., 3., 4. 등)가 함께 제공됩니다. 모델은 이 보기 중에서 정답의 **번호**를 맞춰야 합니다.
    - **주관식 (Open-ended / Short Answer):** "... 의 개념을 설명하시오." 또는 "... 이란 무엇인가?" 와 같이 특정 용어나 개념에 대한 **서술형 답변**을 요구합니다.

## ### 3.2 출력 (Output)

- **파일 형식:** `sample_submission.csv`와 동일한, `id`와 `answer` 두 개의 컬럼을 가진 CSV 파일.
- **답변 형식:**
    - **객관식 문제의 경우:** 모델은 정답이라고 판단되는 보기의 **숫자**를 텍스트 형태로 생성해야 합니다. (예: `"1"`, `"4"`)
    - **주관식 문제의 경우:** 모델은 질문에 대한 **서술형 답변**을 텍스트로 생성해야 합니다.
- **핵심 요구사항:** 모든 답변은 RAG 시스템을 통해 검색된 정보를 근거로 LLM이 **새롭게 생성(Generate)**한 결과물이어야 합니다.

## ### 3.3 평가 지표 (Evaluation Metric)

- **지표:** FSKU 평가지표
- **추정 방식:** 과제의 입출력 형식을 고려할 때, FSKU 지표는 아래 두 방식의 조합일 가능성이 높습니다.
    - **객관식:** **정확도(Accuracy)**. 모델이 출력한 정답 번호와 실제 정답 번호가 일치하는지를 평가합니다.
    - **주관식:** **의미적 유사도(Semantic Similarity)**. 모델이 생성한 서술형 답변과 실제 정답 사이의 의미적 유사도를 측정합니다. (예: ROUGE, BERTScore 등)

---
