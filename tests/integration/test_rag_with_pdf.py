"""
ì‹¤ì œ PDF ë¬¸ì„œë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸ PDFë¥¼ ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸
"""

import sys
import io
import time
import logging
from pathlib import Path
from typing import List, Dict

# UTF-8 ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

# ì‹œìŠ¤í…œ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from packages.preprocessing.pdf_processor_advanced import AdvancedPDFProcessor
from packages.preprocessing.chunker import DocumentChunker
from packages.rag import create_rag_pipeline


def load_pdf_documents(pdf_path: str) -> List[Dict]:
    """PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹"""
    logger.info(f"\nğŸ“„ PDF ë¡œë”©: {pdf_path}")
    
    # PDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    pdf_processor = AdvancedPDFProcessor()
    
    # PDF ì²˜ë¦¬
    try:
        result = pdf_processor.extract_pdf(pdf_path)
        logger.info(f"âœ… PDFì—ì„œ {len(result.page_texts)} í˜ì´ì§€ ì¶”ì¶œ")
        logger.info(f"   ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result.text)} ë¬¸ì")
        
        # í˜ì´ì§€ë³„ ë¬¸ì„œ ìƒì„±
        documents = []
        for page_num, page_text in enumerate(result.page_texts, 1):
            doc = {
                'content': page_text,
                'metadata': {
                    'page': page_num,
                    'source': pdf_path,
                    'has_tables': len(result.tables) > 0
                }
            }
            documents.append(doc)
        
        # ì²˜ìŒ 3í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°
        for i, doc in enumerate(documents[:3]):
            content_preview = doc['content'][:200].replace('\n', ' ') if doc['content'] else ""
            logger.info(f"  í˜ì´ì§€ {i+1}: {content_preview}...")
        
        return documents
    except Exception as e:
        logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []


def chunk_documents(documents: List, chunk_size: int = 512, overlap: int = 50) -> List[Dict]:
    """ë¬¸ì„œë¥¼ ì²­í‚¹"""
    logger.info(f"\nâœ‚ï¸ ë¬¸ì„œ ì²­í‚¹ (chunk_size={chunk_size}, overlap={overlap})")
    
    # ì²­ì»¤ ì´ˆê¸°í™”
    chunker = DocumentChunker(
        chunk_size=chunk_size,
        chunk_overlap=overlap
    )
    
    # ëª¨ë“  ë¬¸ì„œë¥¼ ì²­í‚¹
    all_chunks = []
    for doc in documents:
        content = doc.get('content', '')
        
        if not content:
            continue
            
        # ì²­í‚¹
        chunks = chunker.chunk_document(content)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        page_num = doc.get('metadata', {}).get('page', 0)
        for i, chunk in enumerate(chunks):
            # DocumentChunk ê°ì²´ì¸ ê²½ìš°
            if hasattr(chunk, 'content'):
                chunk_text = chunk.content
            else:
                chunk_text = str(chunk)
            
            chunk_doc = {
                'content': chunk_text,
                'metadata': {
                    'source': 'AI_ë³´ì•ˆ_ê°€ì´ë“œë¼ì¸.pdf',
                    'page': page_num,
                    'chunk_id': f"page_{page_num}_chunk_{i}",
                    'chunk_index': i,
                    'total_chunks': len(chunks)
                }
            }
            all_chunks.append(chunk_doc)
    
    logger.info(f"âœ… {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
    return all_chunks


def test_rag_with_real_pdf():
    """ì‹¤ì œ PDFë¡œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    print("="*60)
    print("ì‹¤ì œ PDF ë¬¸ì„œë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # 1. PDF ë¡œë“œ
    pdf_path = "data/raw/ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf"
    if not Path(pdf_path).exists():
        logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return False
    
    documents = load_pdf_documents(pdf_path)
    if not documents:
        logger.error("PDF ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨")
        return False
    
    # 2. ë¬¸ì„œ ì²­í‚¹
    chunks = chunk_documents(documents, chunk_size=512, overlap=50)
    if not chunks:
        logger.error("ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨")
        return False
    
    # 3. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    logger.info("\nğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
    
    try:
        pipeline = create_rag_pipeline(
            embedder_type="kure",
            retriever_type="vector",  # ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©
            enable_reranking=False  # ë¦¬ë­í‚¹ ë¹„í™œì„±í™” (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        )
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
    
    # 4. ë¬¸ì„œ ì¶”ê°€
    logger.info("\nğŸ“¥ ì§€ì‹ ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€")
    
    try:
        # ì²­í¬ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬
        texts = [chunk['content'] for chunk in chunks]
        metadata = [chunk['metadata'] for chunk in chunks]
        
        # ë°°ì¹˜ë¡œ ë¬¸ì„œ ì¶”ê°€
        start_time = time.time()
        num_added = pipeline.add_documents(
            texts=texts,
            metadata=metadata,
            batch_size=32
        )
        add_time = time.time() - start_time
        
        logger.info(f"âœ… {num_added}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {add_time:.2f}ì´ˆ)")
        logger.info(f"   í‰ê· : {add_time/num_added:.3f}ì´ˆ/ë¬¸ì„œ")
        
        # ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥
        kb_path = "pdf_knowledge_base.pkl"
        pipeline.save_knowledge_base(kb_path)
        logger.info(f"ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥: {kb_path}")
        
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return False
    
    # 5. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    logger.info("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    
    test_queries = [
        "AI ì‹œìŠ¤í…œì˜ ë³´ì•ˆ ìœ„í˜‘ì€ ë¬´ì—‡ì¸ê°€?",
        "ê¸ˆìœµ AIì˜ ë°ì´í„° ë³´í˜¸ ë°©ë²•ì€?",
        "ëª¨ë¸ ê³µê²©ì— ëŒ€í•œ ë°©ì–´ ì „ëµì€?",
        "AI ëª¨ë¸ì˜ ì·¨ì•½ì  í‰ê°€ ë°©ë²•ì€?",
        "ê¸ˆìœµ ë¶„ì•¼ AI ê·œì œ ìš”êµ¬ì‚¬í•­ì€?",
    ]
    
    for query in test_queries:
        logger.info(f"\nì§ˆë¬¸: {query}")
        logger.info("-" * 40)
        
        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            start_time = time.time()
            results = pipeline.retrieve(query, top_k=3)
            search_time = time.time() - start_time
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ ({search_time:.3f}ì´ˆ)")
            
            # ê²°ê³¼ ì¶œë ¥
            if results:
                for i, doc in enumerate(results, 1):
                    score = doc.get('score', 0)
                    content = doc.get('content', '')[:150]
                    metadata = doc.get('metadata', {})
                    page = metadata.get('page', 'N/A')
                    
                    logger.info(f"\n  [{i}] ì ìˆ˜: {score:.4f} | í˜ì´ì§€: {page}")
                    logger.info(f"      {content}...")
            else:
                logger.info("  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = pipeline.generate_context(query, top_k=3, max_length=1000)
            if context:
                logger.info(f"\nìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ (ì²« 300ì):")
                logger.info(f"{context[:300]}...")
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 6. í†µê³„ ì¶œë ¥
    logger.info("\nğŸ“Š íŒŒì´í”„ë¼ì¸ í†µê³„")
    stats = pipeline.get_statistics()
    logger.info(f"  - Embedder: {stats['embedder_model']}")
    logger.info(f"  - Embedding ì°¨ì›: {stats['embedding_dim']}")
    logger.info(f"  - ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {stats['num_documents']}")
    logger.info(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {stats['index_size']}")
    
    return True


def test_specific_questions():
    """íŠ¹ì • ë„ë©”ì¸ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("íŠ¹ì • ë„ë©”ì¸ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    # ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
    kb_path = "pdf_knowledge_base.pkl"
    if not Path(kb_path).exists():
        logger.warning(f"ì§€ì‹ ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € test_rag_with_real_pdf()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ê¸°ì¡´ KB ë¡œë“œ
    pipeline = create_rag_pipeline(
        embedder_type="kure",
        retriever_type="vector",
        knowledge_base_path=kb_path,
        enable_reranking=False
    )
    
    # ë„ë©”ì¸ë³„ ì§ˆë¬¸
    domain_questions = {
        "ë³´ì•ˆ ìœ„í˜‘": [
            "ì ëŒ€ì  ê³µê²©(adversarial attack)ì´ë€?",
            "ë°ì´í„° í¬ì´ì¦ˆë‹ ê³µê²© ë°©ì–´ ë°©ë²•ì€?",
            "ëª¨ë¸ ì¶”ì¶œ ê³µê²©ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì€?"
        ],
        "ê·œì œ ì¤€ìˆ˜": [
            "AI ì‹œìŠ¤í…œì˜ ì„¤ëª…ê°€ëŠ¥ì„± ìš”êµ¬ì‚¬í•­ì€?",
            "ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•œ AI ì„¤ê³„ ì›ì¹™ì€?",
            "ê¸ˆìœµ AIì˜ ê°ì‚¬ ì¶”ì  ìš”êµ¬ì‚¬í•­ì€?"
        ],
        "ëª¨ë¸ ë³´ì•ˆ": [
            "ëª¨ë¸ ì•”í˜¸í™” ê¸°ë²•ì€?",
            "ì—°í•©í•™ìŠµì˜ ë³´ì•ˆ ì´ì ì€?",
            "ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ì ìš© ë°©ë²•ì€?"
        ]
    }
    
    for domain, questions in domain_questions.items():
        logger.info(f"\n[{domain}]")
        for question in questions:
            logger.info(f"\nQ: {question}")
            
            # ê²€ìƒ‰
            results = pipeline.retrieve(question, top_k=1)
            if results:
                best_match = results[0]
                logger.info(f"A: {best_match.get('content', '')[:200]}...")
                logger.info(f"   (ì ìˆ˜: {best_match.get('score', 0):.4f})")
    
    return True


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # 1. ì‹¤ì œ PDFë¡œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        success = test_rag_with_real_pdf()
        
        if success:
            # 2. íŠ¹ì • ë„ë©”ì¸ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
            test_specific_questions()
            
            print("\n" + "="*60)
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print("="*60)
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        return success
        
    except KeyboardInterrupt:
        print("\n\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"\n\nì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)