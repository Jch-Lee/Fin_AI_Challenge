#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•œ VL ëª¨ë¸ ì¶”ì¶œ ì‹¤í—˜
ê¸°ì¡´ ê²°ê³¼ ë””ë ‰í† ë¦¬ì— vl_model_v2.txt íŒŒì¼ë§Œ ì¶”ê°€
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import torch
import pymupdf
from PIL import Image
import io
import traceback

def create_advanced_semantic_prompt():
    """Version 1: ìƒì„¸ ê·œì¹™ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸"""
    prompt = """
ì´ ë¬¸ì„œ í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë§Œ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

## ì¶”ì¶œ ìš°ì„ ìˆœìœ„
1ê¸‰ (ë°˜ë“œì‹œ ì¶”ì¶œ):
- ëª¨ë“  ì œëª©ê³¼ ì†Œì œëª©
- ë³¸ë¬¸ ë‹¨ë½ì˜ ì™„ì „í•œ ë¬¸ì¥
- ì •ì˜, ì„¤ëª…, ê²°ë¡ 
- í•µì‹¬ ìˆ˜ì¹˜ì™€ ê·¸ ë§¥ë½

2ê¸‰ (ìš”ì•½í•˜ì—¬ ì¶”ì¶œ):
- ê·¸ë˜í”„/ì°¨íŠ¸ì˜ í•µì‹¬ ë©”ì‹œì§€
- í‘œì˜ ì£¼ìš” ë°ì´í„°
- ì´ë¯¸ì§€ì˜ ì„¤ëª…

3ê¸‰ (ì œì™¸):
- ì¶• ëˆˆê¸ˆê°’, ê²©ìì„  ìˆ«ì
- í˜ì´ì§€ ë²ˆí˜¸, ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€
- ë°˜ë³µë˜ëŠ” ë ˆì´ë¸”

## ì„¸ë¶€ ì²˜ë¦¬ ê·œì¹™

### ğŸ“ˆ ê·¸ë˜í”„/ì°¨íŠ¸ ì²˜ë¦¬
ì¶œë ¥ í˜•ì‹:
### [ì°¨íŠ¸] {ì°¨íŠ¸ ì œëª©}
- **ì¸¡ì • í•­ëª©**: {Yì¶• ë ˆì´ë¸”}
- **ê¸°ê°„/ë²”ìœ„**: {Xì¶• ë²”ìœ„}
- **í•µì‹¬ ë°œê²¬**: {ì£¼ìš” íŠ¸ë Œë“œë‚˜ íŠ¹ì´ì }
- **ì£¼ìš” ìˆ˜ì¹˜**: {ìµœëŒ€/ìµœì†Œ/ë³€í™”ìœ¨ ë“± ì˜ë¯¸ìˆëŠ” ê°’ë§Œ}

### ğŸ“Š í‘œ(Table) ì²˜ë¦¬
- í—¤ë”ëŠ” í•­ìƒ í¬í•¨
- ë°ì´í„°ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ ì„ íƒ:
  a) 5í–‰ ì´í•˜: ì „ì²´ í¬í•¨
  b) 6-10í–‰: ìƒìœ„ 3ê°œ + "... ì™¸ Nê°œ"
  c) 10í–‰ ì´ˆê³¼: ìš”ì•½ í†µê³„ë§Œ

### ğŸ“ í…ìŠ¤íŠ¸ ì²˜ë¦¬
- ì™„ì „í•œ ë¬¸ì¥: ê·¸ëŒ€ë¡œ ìœ ì§€
- ë‚˜ì—´ëœ ë‹¨ì–´/ìˆ«ì: ë¬¸ë§¥ ìˆëŠ” ê²ƒë§Œ ìœ ì§€

### ğŸ–¼ï¸ ì´ë¯¸ì§€/ë‹¤ì´ì–´ê·¸ë¨ ì²˜ë¦¬
![ë‹¤ì´ì–´ê·¸ë¨] {ì„¤ëª…}
- êµ¬ì„±ìš”ì†Œ: {ì£¼ìš” ìš”ì†Œ ë‚˜ì—´}
- ê´€ê³„/íë¦„: {ìš”ì†Œ ê°„ ê´€ê³„ ì„¤ëª…}
- í•µì‹¬ ë©”ì‹œì§€: {ë‹¤ì´ì–´ê·¸ë¨ì´ ì „ë‹¬í•˜ëŠ” í•µì‹¬}

## ê¸ˆì§€ ì‚¬í•­
ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
- ë‹¨ë… ìˆ«ì ë‚˜ì—´ (0, 5, 10, 15, 20...)
- ë‚ ì§œë§Œ ë‚˜ì—´ (1ì›”, 2ì›”, 3ì›”...)
- "ê·¸ë¦¼ 1", "í‘œ 2" ê°™ì€ ì°¸ì¡° ë²ˆí˜¸
- ë²”ë¡€ì˜ ìƒ‰ìƒ ì„¤ëª… (ë¹¨ê°„ìƒ‰, íŒŒë€ìƒ‰...)
- ê²©ìì„ , ì¶• ëˆˆê¸ˆê°’

## ì¶œë ¥ ê²€ì¦
ì¶”ì¶œ í›„ ìê°€ ê²€ì¦:
- ê° ì¶”ì¶œ í•­ëª©ì´ ë…ë¦½ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ìˆëŠ”ê°€?
- ì»¨í…ìŠ¤íŠ¸ ì—†ì´ë„ ì´í•´ ê°€ëŠ¥í•œê°€?
- RAG ê²€ìƒ‰ ì‹œ ìœ ìš©í•œ ì •ë³´ì¸ê°€?
"""
    return prompt

def run_improved_extraction():
    """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì¶”ì¶œ ì‹¤í–‰"""
    
    # ê¸°ì¡´ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì‚¬ìš©
    existing_output_dir = Path('outputs/full_extraction_20250814_052410')
    pdf_path = Path('data/ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf')
    
    if not existing_output_dir.exists():
        print(f"âŒ Error: Directory not found: {existing_output_dir}")
        return
    
    print('ğŸš€ Improved VL Extraction Experiment (Version 2 Prompt)')
    print('=' * 60)
    print(f'ğŸ“„ PDF: {pdf_path}')
    print(f'ğŸ“ Adding results to: {existing_output_dir}')
    
    # PDF ì •ë³´ í™•ì¸
    doc = pymupdf.open(pdf_path)
    total_pages = len(doc)
    doc.close()
    
    print(f'ğŸ“– Total pages: {total_pages}')
    print('=' * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    print('\nğŸ“¦ Loading Qwen2.5-VL model...')
    
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            'Qwen/Qwen2.5-VL-7B-Instruct',
            device_map='auto',
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        processor = AutoProcessor.from_pretrained(
            'Qwen/Qwen2.5-VL-7B-Instruct',
            trust_remote_code=True
        )
        
        print('âœ… Model loaded successfully!')
        
        if torch.cuda.is_available():
            memory = torch.cuda.memory_allocated(0) / 1024**3
            print(f'ğŸ“Š GPU memory used: {memory:.1f} GB')
    
    except Exception as e:
        print(f'âŒ Failed to load model: {e}')
        return
    
    # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    prompt = create_advanced_semantic_prompt()
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = []
    total_v2_chars = 0
    successful_pages = 0
    failed_pages = []
    
    start_time = time.time()
    
    # í˜ì´ì§€ë³„ ì²˜ë¦¬
    for page_num in range(total_pages):
        print(f'\nğŸ“– Processing page {page_num + 1}/{total_pages}...')
        
        page_dir = existing_output_dir / f'page_{page_num+1:03d}'
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ (ê¸°ì¡´ ì´ë¯¸ì§€ ì‚¬ìš©)
            image_path = page_dir / 'original.png'
            if not image_path.exists():
                print(f'  âš ï¸ Original image not found, converting from PDF...')
                doc = pymupdf.open(pdf_path)
                page = doc[page_num]
                pix = page.get_pixmap(dpi=150)
                img_data = pix.tobytes('png')
                doc.close()
                image = Image.open(io.BytesIO(img_data))
                image.save(image_path)
            else:
                image = Image.open(image_path)
            
            # VL ëª¨ë¸ ì¶”ì¶œ (Version 2 í”„ë¡¬í”„íŠ¸)
            print('  - Extracting with improved VL prompt...')
            vl_start = time.time()
            
            try:
                from qwen_vl_utils import process_vision_info
            except ImportError:
                process_vision_info = None
            
            messages = [{
                'role': 'user',
                'content': [
                    {'type': 'image', 'image': image},
                    {'type': 'text', 'text': prompt}
                ]
            }]
            
            text = processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            if process_vision_info:
                image_inputs, video_inputs = process_vision_info(messages)
            else:
                image_inputs = [image]
                video_inputs = []
            
            inputs = processor(
                text=[text],
                images=image_inputs if image_inputs else None,
                videos=video_inputs if video_inputs else None,
                padding=True,
                return_tensors='pt'
            )
            
            if torch.cuda.is_available():
                inputs = {k: v.to('cuda') if hasattr(v, 'to') else v for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            # ì•ˆì „í•œ ë””ì½”ë”©
            if isinstance(inputs, dict):
                input_ids = inputs.get('input_ids', None)
            else:
                input_ids = inputs.input_ids
            
            if input_ids is not None and outputs is not None:
                generated_ids = []
                for inp, out in zip(input_ids, outputs):
                    inp_len = len(inp) if hasattr(inp, '__len__') else inp.shape[0]
                    out_len = len(out) if hasattr(out, '__len__') else out.shape[0]
                    if out_len > inp_len:
                        generated_ids.append(out[inp_len:])
                    else:
                        generated_ids.append(out)
                
                if generated_ids:
                    vl_text_v2 = processor.batch_decode(
                        generated_ids,
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=False
                    )[0]
                else:
                    vl_text_v2 = '[ë””ì½”ë”© ì‹¤íŒ¨]'
            else:
                vl_text_v2 = '[ìƒì„± ì‹¤íŒ¨]'
            
            vl_time = time.time() - vl_start
            
            # Version 2 ê²°ê³¼ ì €ì¥
            (page_dir / 'vl_model_v2.txt').write_text(vl_text_v2, encoding='utf-8')
            vl_v2_chars = len(vl_text_v2)
            total_v2_chars += vl_v2_chars
            
            # ê¸°ì¡´ ê²°ê³¼ì™€ ë¹„êµë¥¼ ìœ„í•´ ì½ê¸°
            pymupdf_chars = 0
            vl_v1_chars = 0
            
            pymupdf_file = page_dir / 'pymupdf.txt'
            if pymupdf_file.exists():
                pymupdf_text = pymupdf_file.read_text(encoding='utf-8')
                pymupdf_chars = len(pymupdf_text)
            
            vl_v1_file = page_dir / 'vl_model.txt'
            if vl_v1_file.exists():
                vl_v1_text = vl_v1_file.read_text(encoding='utf-8')
                vl_v1_chars = len(vl_v1_text)
            
            # ê²°ê³¼ ê¸°ë¡
            reduction_from_v1 = ((vl_v2_chars - vl_v1_chars) / vl_v1_chars * 100) if vl_v1_chars > 0 else 0
            improvement_from_pymupdf = ((vl_v2_chars - pymupdf_chars) / pymupdf_chars * 100) if pymupdf_chars > 0 else 0
            
            result = {
                'page': page_num + 1,
                'pymupdf_chars': pymupdf_chars,
                'vl_v1_chars': vl_v1_chars,
                'vl_v2_chars': vl_v2_chars,
                'v2_vs_v1_change': reduction_from_v1,
                'v2_vs_pymupdf_change': improvement_from_pymupdf,
                'vl_time': vl_time,
                'success': True
            }
            
            results.append(result)
            successful_pages += 1
            
            print(f'  âœ… PyMuPDF: {pymupdf_chars:,} chars')
            print(f'  âœ… VL V1 (original): {vl_v1_chars:,} chars')
            print(f'  âœ… VL V2 (improved): {vl_v2_chars:,} chars')
            print(f'  ğŸ“Š V2 vs V1: {reduction_from_v1:+.1f}%')
            print(f'  ğŸ“Š V2 vs PyMuPDF: {improvement_from_pymupdf:+.1f}%')
            print(f'  â±ï¸ Processing time: {vl_time:.2f}s')
            
        except Exception as e:
            print(f'  âŒ Failed: {e}')
            failed_pages.append(page_num + 1)
            results.append({
                'page': page_num + 1,
                'error': str(e),
                'success': False
            })
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (5í˜ì´ì§€ë§ˆë‹¤)
        if (page_num + 1) % 5 == 0:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print('  ğŸ§¹ GPU memory cleared')
    
    total_time = time.time() - start_time
    
    # ì „ì²´ í†µê³„ ê³„ì‚°
    total_pymupdf = sum(r['pymupdf_chars'] for r in results if r.get('success'))
    total_v1 = sum(r['vl_v1_chars'] for r in results if r.get('success'))
    
    # ìµœì¢… ìš”ì•½
    print('\n' + '=' * 60)
    print('ğŸ“Š FINAL COMPARISON RESULTS')
    print('=' * 60)
    print(f'âœ… Successful pages: {successful_pages}/{total_pages}')
    if failed_pages:
        print(f'âŒ Failed pages: {failed_pages}')
    print(f'\nğŸ“ Total character counts:')
    print(f'  - PyMuPDF: {total_pymupdf:,} chars')
    print(f'  - VL V1 (original prompt): {total_v1:,} chars')
    print(f'  - VL V2 (improved prompt): {total_v2_chars:,} chars')
    print(f'\nğŸ“ˆ Improvements:')
    if total_v1 > 0:
        print(f'  - V2 vs V1: {((total_v2_chars - total_v1) / total_v1 * 100):+.1f}%')
    if total_pymupdf > 0:
        print(f'  - V2 vs PyMuPDF: {((total_v2_chars - total_pymupdf) / total_pymupdf * 100):+.1f}%')
    print(f'\nâ±ï¸ Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)')
    print(f'â±ï¸ Average time per page: {total_time/total_pages:.2f}s')
    
    # ë¹„êµ ìš”ì•½ ì €ì¥
    comparison_summary = {
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'total_pages': total_pages,
        'successful_pages': successful_pages,
        'failed_pages': failed_pages,
        'total_chars': {
            'pymupdf': total_pymupdf,
            'vl_v1_original': total_v1,
            'vl_v2_improved': total_v2_chars
        },
        'improvements': {
            'v2_vs_v1_percent': ((total_v2_chars - total_v1) / total_v1 * 100) if total_v1 > 0 else 0,
            'v2_vs_pymupdf_percent': ((total_v2_chars - total_pymupdf) / total_pymupdf * 100) if total_pymupdf > 0 else 0,
            'v2_vs_v1_chars': total_v2_chars - total_v1,
            'v2_vs_pymupdf_chars': total_v2_chars - total_pymupdf
        },
        'total_time': total_time,
        'average_time_per_page': total_time / total_pages,
        'page_results': results
    }
    
    summary_file = existing_output_dir / 'comparison_summary_v2.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(comparison_summary, f, indent=2, ensure_ascii=False)
    
    print(f'\nğŸ“ Results added to: {existing_output_dir}')
    print(f'ğŸ“„ Comparison summary: {summary_file}')
    
    return existing_output_dir

if __name__ == '__main__':
    run_improved_extraction()