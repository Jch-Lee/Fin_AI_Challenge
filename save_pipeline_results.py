"""
pipeline_results í˜•ì‹ìœ¼ë¡œ RAG ì‹¤í—˜ ê²°ê³¼ ì €ì¥
ì˜¤ëŠ˜ ì‹¤í–‰í•œ PDF RAG í…ŒìŠ¤íŠ¸ì˜ ì¤‘ê°„ ê³¼ì •ì„ ì²´ê³„ì ìœ¼ë¡œ ì €ì¥
"""

import sys
import io
import json
import pickle
import numpy as np
from pathlib import Path
from datetime import datetime
import shutil

# UTF-8 ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ì‹œìŠ¤í…œ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from packages.preprocessing.pdf_processor_advanced import AdvancedPDFProcessor
from packages.preprocessing.chunker import DocumentChunker
from packages.rag import create_rag_pipeline


def create_pipeline_results_folder():
    """ìƒˆë¡œìš´ pipeline_results í´ë” ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"pipeline_results_{timestamp}"
    folder_path = Path(folder_name)
    folder_path.mkdir(exist_ok=True)
    return folder_path


def save_step_1_pdf_extraction(output_dir: Path, pdf_path: str):
    """Step 1: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    print("\n[Step 1] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    
    processor = AdvancedPDFProcessor()
    result = processor.extract_pdf(pdf_path)
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
    text_file = output_dir / "01_extracted_text.txt"
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(result.text)
    
    print(f"âœ… ì €ì¥: {text_file}")
    print(f"   - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result.text)} ë¬¸ì")
    print(f"   - í˜ì´ì§€ ìˆ˜: {len(result.page_texts)}")
    
    return result


def save_step_2_chunking(output_dir: Path, pdf_result):
    """Step 2: í…ìŠ¤íŠ¸ ì²­í‚¹"""
    print("\n[Step 2] í…ìŠ¤íŠ¸ ì²­í‚¹")
    
    chunker = DocumentChunker(chunk_size=512, chunk_overlap=50)
    all_chunks = []
    
    for page_num, page_text in enumerate(pdf_result.page_texts, 1):
        if not page_text:
            continue
        
        chunks = chunker.chunk_document(page_text)
        for i, chunk in enumerate(chunks):
            if hasattr(chunk, 'content'):
                chunk_text = chunk.content
            else:
                chunk_text = str(chunk)
            
            chunk_data = {
                'text': chunk_text,
                'page': page_num,
                'chunk_index': i,
                'metadata': {
                    'source': 'ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf',
                    'page': page_num,
                    'chunk_id': f"page_{page_num}_chunk_{i}"
                }
            }
            all_chunks.append(chunk_data)
    
    # ì²­í¬ ì €ì¥
    chunks_file = output_dir / "02_chunks.json"
    with open(chunks_file, 'w', encoding='utf-8') as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥: {chunks_file}")
    print(f"   - ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}")
    
    return all_chunks


def save_step_3_embeddings(output_dir: Path, chunks):
    """Step 3: ì„ë² ë”© ìƒì„±"""
    print("\n[Step 3] ì„ë² ë”© ìƒì„±")
    
    # RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    pipeline = create_rag_pipeline(
        embedder_type="kure",
        retriever_type="vector",
        enable_reranking=False
    )
    
    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    texts = [chunk['text'] for chunk in chunks]
    
    # ì„ë² ë”© ìƒì„±
    embeddings = pipeline.embedder.embed_batch(texts, batch_size=32)
    embeddings_array = np.array(embeddings)
    
    # ì„ë² ë”© ì €ì¥
    embeddings_file = output_dir / "03_embeddings.npy"
    np.save(embeddings_file, embeddings_array)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'model': pipeline.embedder.model_name,
        'total_chunks': len(chunks),
        'embedding_dimension': embeddings_array.shape[1],
        'embedding_shape': embeddings_array.shape
    }
    
    metadata_file = output_dir / "03_embedding_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… ì €ì¥: {embeddings_file}")
    print(f"âœ… ì €ì¥: {metadata_file}")
    print(f"   - ì„ë² ë”© shape: {embeddings_array.shape}")
    print(f"   - ëª¨ë¸: {pipeline.embedder.model_name}")
    
    return pipeline, embeddings_array


def save_step_4_index(output_dir: Path, pipeline, embeddings, chunks):
    """Step 4: FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
    print("\n[Step 4] FAISS ì¸ë±ìŠ¤ êµ¬ì¶•")
    
    # ë¬¸ì„œ ì¶”ê°€
    texts = [chunk['text'] for chunk in chunks]
    metadata = [chunk['metadata'] for chunk in chunks]
    
    # ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€ (ì´ë¯¸ ì„ë² ë”©ì€ ìƒì„±ë¨)
    documents = []
    for i, (text, embedding, meta) in enumerate(zip(texts, embeddings, metadata)):
        doc = {
            "id": i,
            "content": text,
            "embedding": embedding,
            "metadata": meta
        }
        documents.append(doc)
    
    pipeline.knowledge_base.add_documents(embeddings, documents)
    
    # FAISS ì¸ë±ìŠ¤ ì €ì¥
    index_file = output_dir / "04_faiss_index.bin"
    import faiss
    faiss.write_index(pipeline.knowledge_base.index, str(index_file))
    
    print(f"âœ… ì €ì¥: {index_file}")
    print(f"   - ì¸ë±ìŠ¤ í¬ê¸°: {pipeline.knowledge_base.index.ntotal}")
    
    return pipeline


def save_step_5_search(output_dir: Path, pipeline, test_question):
    """Step 5: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("\n[Step 5] ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    results = pipeline.retrieve(test_question, top_k=5)
    
    # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
    search_results = {
        'question': test_question,
        'top_k': 5,
        'results': []
    }
    
    for i, doc in enumerate(results):
        search_results['results'].append({
            'rank': i + 1,
            'score': float(doc.get('score', 0)),
            'content': doc.get('content', '')[:200],
            'metadata': doc.get('metadata', {})
        })
    
    search_file = output_dir / "05_search_results.json"
    with open(search_file, 'w', encoding='utf-8') as f:
        json.dump(search_results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥: {search_file}")
    print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(results)}")
    
    return results


def save_step_6_prompt(output_dir: Path, pipeline, question, search_results):
    """Step 6: í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    print("\n[Step 6] í”„ë¡¬í”„íŠ¸ ìƒì„±")
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = pipeline.generate_context(question, top_k=5, max_length=2000)
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""ë‹¹ì‹ ì€ ê¸ˆìœµ ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

=== ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ===
{context}

=== ì§ˆë¬¸ ===
{question}

=== ë‹µë³€ ===
"""
    
    prompt_file = output_dir / "06_generated_prompt.txt"
    with open(prompt_file, 'w', encoding='utf-8') as f:
        f.write(prompt)
    
    print(f"âœ… ì €ì¥: {prompt_file}")
    print(f"   - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
    
    return prompt


def save_step_7_answer(output_dir: Path, question, prompt):
    """Step 7: ë‹µë³€ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)"""
    print("\n[Step 7] ë‹µë³€ ìƒì„±")
    
    # ì‹¤ì œ LLMì´ ì—†ìœ¼ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜
    answer = {
        'question': question,
        'prompt_length': len(prompt),
        'generated_answer': "ê¸ˆìœµ ê¸°ê´€ì˜ AI ì‹œìŠ¤í…œ ë³´ì•ˆì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¡°ì¹˜ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤:\n\n1. ë°ì´í„° ë³´ì•ˆ: ê°œì¸ì •ë³´ ì•”í˜¸í™”, ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬\n2. ëª¨ë¸ ë³´ì•ˆ: ì ëŒ€ì  ê³µê²© ë°©ì–´, ëª¨ë¸ ì¶”ì¶œ ë°©ì§€\n3. ì‹œìŠ¤í…œ ë³´ì•ˆ: ì¹¨ì… íƒì§€, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\n4. ê·œì œ ì¤€ìˆ˜: ê¸ˆìœµìœ„ì›íšŒ ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜, ì •ê¸° ê°ì‚¬",
        'generation_timestamp': datetime.now().isoformat(),
        'model': "ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ LLM ë¯¸ì‚¬ìš©)",
        'note': "ì‹¤ì œ LLM ì—°ë™ ì‹œ ì´ ë¶€ë¶„ì´ ì‹¤ì œ ë‹µë³€ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤"
    }
    
    answer_file = output_dir / "07_generated_answer.json"
    with open(answer_file, 'w', encoding='utf-8') as f:
        json.dump(answer, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥: {answer_file}")
    
    return answer


def save_pipeline_summary(output_dir: Path, pdf_path: str, chunks, embeddings_shape, question):
    """íŒŒì´í”„ë¼ì¸ ìš”ì•½ ì •ë³´ ì €ì¥"""
    print("\n[Summary] íŒŒì´í”„ë¼ì¸ ìš”ì•½")
    
    summary = {
        'experiment_timestamp': datetime.now().isoformat(),
        'steps_completed': 7,
        'pdf_file': pdf_path,
        'total_chunks': len(chunks),
        'embedding_model': 'nlpai-lab/KURE-v1',
        'embedding_dimension': embeddings_shape[1],
        'embedding_shape': list(embeddings_shape),
        'test_question': question,
        'retrieval_top_k': 5,
        'pipeline_success': True,
        'output_directory': str(output_dir)
    }
    
    summary_file = output_dir / "pipeline_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥: {summary_file}")
    
    return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*60)
    print("Pipeline Results í˜•ì‹ìœ¼ë¡œ RAG ì‹¤í—˜ ê²°ê³¼ ì €ì¥")
    print("="*60)
    
    # ì„¤ì •
    pdf_path = "data/raw/ê¸ˆìœµë¶„ì•¼ AI ë³´ì•ˆ ê°€ì´ë“œë¼ì¸.pdf"
    test_question = "ê¸ˆìœµ AI ì‹œìŠ¤í…œì˜ ë³´ì•ˆì„ ìœ„í•´ ì–´ë–¤ ì¡°ì¹˜ë“¤ì´ í•„ìš”í•œê°€ìš”?"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = create_pipeline_results_folder()
    print(f"\nğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    try:
        # Step 1: PDF ì¶”ì¶œ
        pdf_result = save_step_1_pdf_extraction(output_dir, pdf_path)
        
        # Step 2: ì²­í‚¹
        chunks = save_step_2_chunking(output_dir, pdf_result)
        
        # Step 3: ì„ë² ë”©
        pipeline, embeddings = save_step_3_embeddings(output_dir, chunks)
        
        # Step 4: ì¸ë±ìŠ¤
        pipeline = save_step_4_index(output_dir, pipeline, embeddings, chunks)
        
        # Step 5: ê²€ìƒ‰
        search_results = save_step_5_search(output_dir, pipeline, test_question)
        
        # Step 6: í”„ë¡¬í”„íŠ¸
        prompt = save_step_6_prompt(output_dir, pipeline, test_question, search_results)
        
        # Step 7: ë‹µë³€
        answer = save_step_7_answer(output_dir, test_question, prompt)
        
        # Summary
        summary = save_pipeline_summary(output_dir, pdf_path, chunks, embeddings.shape, test_question)
        
        print("\n" + "="*60)
        print("âœ… ëª¨ë“  íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}")
        print("="*60)
        
        # íŒŒì¼ ëª©ë¡ ì¶œë ¥
        print("\nìƒì„±ëœ íŒŒì¼:")
        for file in sorted(output_dir.glob("*")):
            size = file.stat().st_size
            print(f"  - {file.name} ({size:,} bytes)")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)